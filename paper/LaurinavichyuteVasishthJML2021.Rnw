\documentclass[doc,floatsintext]{apa6}

\usepackage{doi}
\usepackage{longtable}
\usepackage[american]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[backend=biber,useprefix=true,style=apa,url=false,doi=false,sorting=nyt,eprint=false]{biblatex}

\usepackage{fancyvrb}

\usepackage{newfloat}
\DeclareFloatingEnvironment[
%    fileext=los,
%    listname=List of Schemes,
%    name=Listing,
%    placement=!htbp,
%    within=section,
]{listing}

\usepackage{lscape} % landscape table
\usepackage{longtable}
\usepackage{threeparttablex}
\usepackage{booktabs}
\usepackage{multirow} % multirows in tables 
\usepackage{bigdelim} % curly braces in table
\usepackage{xcolor,colortbl}

\usepackage{mathtools}
\makeatletter
 
\newcommand{\explain}[2]{\underset{\mathclap{\overset{\uparrow}{#2}}}{#1}}
\newcommand{\explainup}[2]{\overset{\mathclap{\underset{\downarrow}{#2}}}{#1}}
 
\makeatother

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{microtype}
\usepackage[american]{babel}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage{setspace}
\usepackage{geometry}
%\usepackage{lineno,clipboard}
%\newclipboard{reviews}
%\openclipboard{reviews}

%\newcommand{\revised}[1]{{\color{black}{#1}}}

\usepackage{tikz}
\usetikzlibrary{arrows.meta}
\tikzset{%
  >={Latex[width=2mm,length=2mm]},
  % Specifications for style of nodes:
            base/.style = {rectangle, rounded corners, draw=black,
                           minimum width=4cm, minimum height=1cm,
                           text centered, font=\sffamily},
  activityStarts/.style = {base, fill=blue!30},
       startstop/.style = {base, fill=red!30},
    activityRuns/.style = {base, fill=green!30},
         process/.style = {base, minimum width=2.5cm, fill=orange!15,
                           font=\ttfamily},
}

\usepackage{gb4e}


\DeclareLanguageMapping{american}{american-apa}
\addbibresource{LaurinavichyuteVasishthJML2021.bib}


\leftheader{Laurinavichyute and Vasishth}
\title{The (ir)reproducibility of published analyses: A case study of 57 JML articles published between 2019 and 2021}
\shorttitle{(Ir)reproducibility in psychology and psycholinguistics}

\twoauthors{Anna Laurinavichyute}{Shravan Vasishth}

\twoaffiliations{Department of Linguistics, University of Potsdam, Potsdam, Germany}{Department of Linguistics, University of Potsdam, Potsdam, Germany}

\authornote{Please send correspondence to anna.laurinavichyute@uni-potsdam.de.}
%\note{\today}

\journal{Submitted to the Journal of Memory and Language} 
\volume{Version dated 30 September 2021} 

\keywords{open data; reproducible statistical analyses; reproducibility; open science; meta-research; journal policy}

<<setup,include=FALSE,cache=FALSE,echo=FALSE>>=
library('knitr')
library('coda') 
library('plyr') 
library('ggplot2') 
library('xtable')
library('dplyr')
library('SIN')
library('papaja') 
library('tidyverse') 
library('tidybayes')
library('rstantools')
library('brms')
library('riverplot') 
library('boot')
@

<<loaddata, include=FALSE>>=
data <- read.delim("../data/overview_numeric_anon.csv", sep=";", 
                   header=TRUE, 
                   na.strings=c(""))%>%
  select(-Processed) 

sbst <- data 

papers_with_data <- sbst %>% filter(Data_accessible == "yes") 
papers_with_data_no_code <- sbst %>% filter(Data_accessible == "yes", Code == "no") 
papers_with_code <- papers_with_data %>% filter(Code == "yes") 
@



\abstract{In 2019 the Journal of Memory and Language instituted an open data and code policy; this policy requires that, as a rule, code and data be released at the latest upon publication. Does this policy lead to reproducible results? We looked at whether 57 papers published between 2019 and 2021 were reproducible, in the sense that the published summary statistics should be possible to regenerate given the data, and given the code, when code was provided. We found that for \Sexpr{sbst %>% filter(Data_accessible == "no") %>% nrow()} out of the 57 papers, data sets were inaccessible; \Sexpr{papers_with_code %>% nrow()} of the remaining \Sexpr{papers_with_data %>% nrow()} papers provided code, of which \Sexpr{papers_with_code %>% filter(Reproducible == "yes") %>% nrow()} were reproducible. Of the \Sexpr{papers_with_data_no_code %>% nrow()} papers that did not provide code, one was reproducible. Overall, the reproducibility rate was about 30\%. This estimate is similar to the ones reported for psychology, economics, and other areas, but it is probably possible to do better. We provide some suggestions on how reproducibility can be improved in future work.} 

\DeclareUnicodeCharacter{0301}{\'{e}}
 
\begin{document}

\maketitle


\section{Introduction}

Being able to build on existing knowledge is key to scientific progress. A prerequisite for such incremental knowledge gain is that existing research be reliable, in the sense that we can be relatively confident that the claimed findings in a paper reflect something that is true about the phenomenon being investigated. The replicability of published claims is a key component of reliability \parencite{munafo2017manifesto}. Here, replicability can be broadly understood to mean that one can obtain the same or similar conclusions as in a published result when one repeats the same experiment with new participants.

In recent years, the replicability of apparently well-established results in psychology has been called into question \parencite[e.g.,][]{anderson2015estimating}. Since then, within psychology and psycholinguistics several failed replication attempts of well-known claims have been reported \parencites[e.g.,][]{klein2014investigating, hagger2016multilab, klein2018many, nieuwland2018large, stack2018failure,jager2020interference, vasishth2018statistical, mertzen2020cross}. Such failures to replicate raise important questions about the extent of the non-replicability problem in psycholinguistics and related areas. Partly in response to this concern, an upcoming special issue of the \emph{Journal of Memory and Language} focuses on the replication of influential findings in memory and language. This move from the journal is a clear signal to the field that replication has an important role to play in scientific progress. 

Replicability is an important aspect of assessing the reliability of a particular finding. However, if one fails to replicate a study, it is often difficult to work out exactly why the failure to replicate happened.
This is because the failure may be driven by many factors that are external to the research question under investigation: differences in the population and/or language studied, the natural variability in the dependent variable, lab settings, equipment, and protocols can come together to lead to very different outcomes. Indeed, it is possible that when it comes to studying subtle and highly variable aspects of human behavior, replicability may be an unattainable goal.

While replicability may be to a great extent beyond the control of the researcher, another important aspect of reliability depends to a greater extent on the researcher. This fundamental aspect is the reproducibility of published claims. Reproducibility refers to the ability to regenerate the key summary statistics (e.g., means, standard errors, t-, F-, or p-values) reported in a paper using the data (and code) provided with the paper \parencites{nuijten2018verify,lebel2018unified,stodden2018empirical}. Given some fixed data, if the key summary statistics that the statistical inference is based on are not reproducible, it is unclear what a replication attempt should aim to replicate.

On the face of it, reproducibility might seem to be an easily attainable goal: is one not bound to obtain the same results on the original data set if one simply re-runs the data analysis code again? In recent years, researchers in psychology and other areas have come to learn that success in reproducing the original results is not a given. Researchers in different fields have been investigating the reproducibility of published claims. Some examples are political science \parencite{stockemer2018data}, economics \parencite{chang2015economics}, biomedical science \parencite{naudet2018data}, machine learning \parencite{raff2019step}, and hydrology \parencite{stagge2019assessing}. Across all these studies, the reproducibility rate has been reported to be around 30\%, ranging from 17\% in political science \parencite{eubank2016lessons} to 37\% in economics \parencite{chang2015economics}, with some rare exceptions (58\% for registered reports in psychology, \cite{obels2020analysis}; 82\% in biomedicine, \cite{naudet2018data}). In recognition of this irreproducibility crisis, the journal \emph{Cortex} has introduced a new article type, the verification report. The sole purpose of this article type is to repeat the original analyses or report new analyses of the original data set \parencite{chambers2020verification}.

One of the most basic barriers to reproducibility is data unavailability: if the original data set is not openly available \parencite[which is often the case, see][]{wicherts2006poor, vanpaemel2015we}, independent analysis cannot be performed. In response to this challenge, many journals in psychology and linguistics have adopted a mandatory data sharing policy. Examples are \emph{Cognition}, \emph{Cortex}, \emph{Collabra:Psychology}, \emph{Open Mind}, \emph{Glossa Psycholinguistics}, \emph{Journal of Cognition}, and \emph{Computational Brain and Behavior}. More than a thousand  journals from a wide range of fields are signatories to the Transparency and Openness Promotion (TOP) guidelines from the Center for Open Science (https://www.cos.io/initiatives/top-guidelines).

The introduction of such guidelines and policies has been effective in increasing the proportion of open data sets accompanying articles \parencite{nuijten2017journal, hardwicke2018data}. However, although ensuring data availability is a necessary condition for reproducibility, it may not be sufficient: \textcite{towse2021opening} analyzed the quality of open data sets across psychological journals. They report that 51\% of the surveyed data sets were not sufficiently documented or did not contain all the data needed to reproduce the reported analyses, and 68\% were archived in a way that limits reusability. Examples of non-reusability were data that were not machine-readable or  relied on proprietary software \parencite[for a similar point, see also][]{hardwicke2021analytic}.

The \emph{Journal of Memory and Language} was among the first linguistics-oriented journals to adopt, in 2019, the mandatory data sharing policy, thus making the first step on the path to ensuring reproducibility of published findings. The aim of the present work is to investigate the reproducibility of papers published in the \emph{Journal of Memory and Language} after this open data policy was instituted. The results of the  analyses are reported below.

\section{Evaluation of the reproducibility of JML articles (2019-2021)}


\subsubsection{Material}

The editor-in-chief (Professor Kathleen Rastle) gave us the titles of 57 papers reporting quantitative experiments that were published after data sharing was made mandatory in JML in 2019. 

The list of papers that were surveyed is as follows: \textcite{samuel2020psycholinguists, skrzypulec2020nonlinear, lelonkiewicz2020morphemes, chan2020does, brainerd2020norming, avetisyan2020does, ahn2020retrieval, troyer2020catch, hollis2020delineating, johns2020production, brysbaert2019many, nooteboom2020repairing, bangerter2020lexical, gagne2020buttercup, mckinley2020role, brothers2021word, diez2020linguistic, brainerd2020explaining, brandt2020computational, villani2021sensorimotor, fox2020accounting, schubert2020reading, collins2020minerva, yang2020origins, snefjella2020emotion, isarida2020video, siew2021syllable, hwang2019cumulative, brainerd2019super, osth2020global, chetail2020graphemic, gunther2020immediate, fellman2020role, tirso2020taking, bristol2020epistemic, floccia2020translation, corps2020top, burki2020did, kaula2020priming, snell2020story, humphreys2020semantic, gunther2020semantic, li2020does, saito2020domain, fujita2020reanalysis, boyce2020maze, falandays2020long, ambrus2020less, hesse2020scalar, garnham2020anticipating, liang2021initial, reifegerste2020effects, lauro2020bilingual, tsuboi2020rethinking, jager2020interference, siegelman2020individual, brewer2021discrepant}.

\subsection{Methods}

We downloaded any available material and attempted to reproduce the analysis for every paper using the description in the paper itself, and the analysis code, if this was provided. 

Data and code were labeled as being accessible when at least some subset (but not necessarily all) of the data described in the paper or code for performing the analysis had been made available. We additionally annotated whether the code was present, the analysis was pre-registered, and whether data was accompanied by a readme file or the like explaining what variable names and their values in the data file stand for. These factors could potentially affect reproducibility: the analysis code documents all the analysis steps; the readme file identifies the explanatory and dependent variables; and a pre-registration may be associated with a more reproducible analysis code (for example, \textcite{obels2020analysis} reported a relatively high reproducibility rate, 58\%, for registered reports in psychology).
   
Papers were labeled as reproducible if all the reported analyses could be reproduced exactly, including reported means (except for the edges of Bayesian credible intervals, where minor fluctuations are expected). If we could not reproduce the exact numbers, even when all the results reported as significant remained significant, the paper was considered to be not reproducible. The reason we focus on the reproducibility of the summary statistics and not whether an effect was significant or not is that statistcal significance per se is not a very informative result, unless the power properties of the experimental design are also known \parencites{gelmancarlin, JaegerEngelmannVasishth2017, vasishth2018statistical, jager2020interference, VasishthGelman2021}.
   
The exact effect estimates from an analysis could be not reproducible for a number of reasons, including updates to the software used to run the analysis, using a updated version of the optimizer used for a linear mixed model, etc. Obviously, the fact that a study was not reproducible does not necessarily say anything about the quality of scientific evidence presented in the study, but rather about the quality of data and analysis presentation. 

\subsection{Descriptive statistics}

\begin{figure}[!htbp]
\centering
<<summaryfig,echo=FALSE,fig.height=2>>=
data_plot <- data %>%
  rename("Pre-registered" = "Pre.registered",
         "Data accessible" = "Data_accessible") %>%
  select(-Paper.ID, -Assigned, -Reproducible)

data_long <- gather(data=data_plot, key=condition, value=y,
                    `Data accessible`:Readme,
                    factor_key=TRUE)
data_long$y<-factor(data_long$y)

data_long %>%
  count(condition,y) %>%
  pivot_wider(names_from = y, values_from = n)%>%
  reshape2::melt(id = "condition") %>%
  ggplot(aes(x = condition, y = value,
                fill = variable, label = value)) +
  geom_bar(stat = "identity") +
  geom_text(size=4, position = position_stack(vjust=0.5)) +
  coord_flip() + theme_bw() +
  scale_fill_grey(start = 0.9, end = 0.6) +
  xlab("") + ylab("") +
  theme(legend.title = element_blank(),
        text = element_text(size =15))
@
\caption{A summary showing the number of papers (a) for which the data were accessible, (b) which were pre-registered, (c) for which the code was available, and (d) which had a readme file or the like that provided some documentation.}\label{fig:summaryplot}
\end{figure}


The key properties of the data and code release that may affect reproducibility are summarized in Figure \ref{fig:summaryplot}. Of all papers that entered the analysis, \Sexpr{papers_with_data %>% nrow()}  (\Sexpr{100 * round(papers_with_data %>% nrow() / sbst %>% nrow(), 2)}\%) had downloadable data sets. The data were inaccessible for \Sexpr{sbst %>% filter(Data_accessible == "no") %>% nrow()} papers. 
The reasons for data unavailability were the following:

\begin{itemize}
\item 
The data were published on the journal website, which is inaccessible to institutions that do not have subscription for Elsevier journals. In contrast to manuscripts, which can be legally accessed as preprints on archives like arXiv or PsyArXiv, or on author's personal websites, these data sets are truly inaccessible for many people.
\item
The data cannot be retrieved from the link provided. The reasons were diverse: DOI not found, the page does not exist, the link leads to a university repository that requires a log-in using the university account, etc. 
\item
Two papers stated that ``data will be available upon request''; for one request, we received data for two experiments out of four, along with a promise to provide the rest of the data if we request it; the other request was met with a question about the planned use for the requested data. In this latter case, we did not reveal that we were carrying out a reproducibility check, and marked this data set as inaccessible, since the journal policy states that data should be made available unconditionally. 
\end{itemize}

Of \Sexpr{papers_with_data %>% nrow()} papers with accessible data,
\Sexpr{papers_with_code %>% nrow()} (\Sexpr{100 * round(papers_with_code %>% nrow() / papers_with_data %>% nrow(), 2)}\%) also shared analysis code.

\subsection{Results}

Only \Sexpr{papers_with_data_no_code %>% filter(Reproducible == "yes") %>% nrow()} of \Sexpr{papers_with_data_no_code %>% nrow()} papers that shared data but not analysis code (\Sexpr{100 * round(papers_with_data_no_code %>% filter(Reproducible == "yes") %>% nrow() / papers_with_data_no_code %>% nrow(), 2)}\%) could be fully reproduced. Of \Sexpr{papers_with_code %>% nrow()} papers that shared both data and analysis code, 
\Sexpr{papers_with_code %>% filter(Reproducible == "yes") %>% nrow()} papers (\Sexpr{100 * round(papers_with_code %>% filter(Reproducible == "yes") %>% nrow() / papers_with_code %>% nrow(), 2)}\%) could be fully reproduced. Of all the papers that had accessible data, \Sexpr{sbst %>% filter (Reproducible == "yes") %>% nrow()} (\Sexpr{100 * round(papers_with_data %>% filter (Reproducible == "yes") %>% nrow() / papers_with_data %>% nrow(),2)}\%) could be fully reproduced. If we consider the reproducibility rate for all papers that entered the analysis, \Sexpr{100 * round(sbst %>% filter (Reproducible == "yes") %>% nrow() / sbst  %>% nrow(),2)}\% could be fully reproduced. Figure \ref{fig:reproducibility} summarizes the breakdown of reproducible papers depending on the presence of analysis code.

<<reproducibilityfig, include = FALSE, echo=FALSE, message=FALSE>>=
ifelse(!dir.exists("images"), dir.create("images"), FALSE)

myedges <- data.frame(ID = 1:8,
                   N1 = c("Not reproduced", "Reproduced",  "Not reproduced", 
                          "Reproduced",  "No code", "Code","No data", "Data"),
                   Value = c(13,16,17,1,18,29,10,47))

myedges$N1 <- paste(myedges$N1, "\n", paste0(myedges$Value))
myedges$N2 <- c(rep("Code \n 29", 2), 
                rep("No code \n 18", 2), 
                rep("Data \n 47", 2), 
                rep(paste("All surveyed papers \n", 57), 2))


mynodes <- data.frame(ID=c(myedges$N1, paste("All surveyed papers \n", 57)), 
                    x=c(1,    1 ,  1,   1,  2, 2, 3,   3, 4), 
                    y=c(5.25, 3.5, 9, 7.1,  8, 5, 4, 6.5, 5))

palette <- c("gray", "#EE6677", "gray", 
             "#4477AA", "#4477AA", "#EE6677", 
             "gray", "#AA3377", "#AA3377")

mystyles = lapply(mynodes$y, function(n) {
  list(col = palette[n])
})

for(i in 1:length(palette)){
  mystyles[[i]]$col<-palette[i]
}

names(mystyles) <- mynodes$ID

custom.style <- riverplot::default.style()
custom.style$textcex <- 1.8

png("images/River.png", units="in", res=120, height=10, width=10)
myriv <- makeRiver(nodes=mynodes, edges=myedges, node_styles=mystyles)
plot(myriv, default_style=custom.style, nodewidth=2, plot_area=.88)
title(ylab="Summary of reproducibility assessment",
      cex.lab=2, line = -1)
dev.off()
@

\begin{figure}[!htbp]
\centering
\includegraphics[scale=0.35,angle=-90]{images/River.png}
\caption{Summary of the reproducibility rates of the 57 articles considered.}\label{fig:reproducibility}
\end{figure}

\subsubsection{Modeling}
<<modelingpriors, include=FALSE, echo=FALSE>>=
data <- read.csv("../data/overview_numeric_anon.csv", sep = ";", 
                 stringsAsFactors = FALSE) %>%
  select(-Processed) 

data[data=="yes"] <- 1
data[data=="no"] <- 0

data <- data %>%
    mutate_if(is.character,as.numeric)

data_filtered <- data %>%
  filter(Data_accessible == 1)

### Bayesian model
priors <- c(prior(normal(0, 1.2), class = Intercept),          
            prior(normal(0, 1.5), class = b)) 
@

<<computepriorpred,echo=FALSE,include=FALSE,cache=TRUE>>=
set.seed(14)
model_priorpred <- brm(Reproducible ~ Readme + Pre.registered + Code, 
             data = data_filtered, family="bernoulli",
             prior = priors,
             sample_prior = "only"
             )
@

<<computepriorpred2,echo=FALSE>>=
alpha_samples <- as_draws_df(model_priorpred)$b_Intercept
av_accuracy <- plogis(alpha_samples)

int_summary<-c(mean = mean(av_accuracy), quantile(av_accuracy, c(0.025, 0.975)))

beta_samples <- as_draws_df(model_priorpred)$b_Code

## the prior predicted effect of code present:
effect_code <-  plogis(alpha_samples + beta_samples) - plogis(alpha_samples)

effect_code_summary<-c(mean = mean(effect_code), quantile(effect_code,
c(0.025, 0.975)))
@

<<modeling, include=FALSE, echo=FALSE>>=
model_treatment <- brm(Reproducible ~ Readme + Pre.registered + Code, 
             data = data_filtered, family="bernoulli",
             prior = priors, file = "model_inf_priors")

# computing credible interval for Code on % scale
model_treatment %>%
  spread_draws(b_Intercept, b_Code) %>%
  mutate(code=plogis(b_Intercept + b_Code)-plogis(b_Intercept)) -> x

# computing credible interval for the intercept on % scale
model_treatment %>%
  spread_draws(b_Intercept) %>%
  mutate(intercept=plogis(b_Intercept)) -> intercept
@


We investigated whether the probability of an analysis being reproducible is affected by the availability of code, a readme file or the like, and whether the study was preregistered. This analysis was carried out via a logistic multiple regression using the \texttt{brms} package \parencite{burkner2017brms}. The absence of a particular feature was coded as 0, and presence of that feature as 1, so that the intercept of the model corresponds to the estimated probability of reproducing the analysis for which only data, but not code, readme, or pre-registration are available. A $\mathcal{N}(0,1.2)$ regularizing prior \parencite{NicenboimEtAlBayes2021} was defined for the intercept; this allows the prior probability of reproducibility given no documentation, no pre-registration, and no code to lie anywhere between \Sexpr{round(int_summary[2],2)*100}\% and \Sexpr{round(int_summary[3],2)*100}\% with 95\% probability (mean: \Sexpr{round(int_summary[1],2)*100}\%). The prior for the slopes was $\mathcal{N}(0, 1.5)$; this implies that the prior probability of how reproducibility could change when analysis  code is present lies between \Sexpr{round(effect_code_summary[2],2)*100}\% and \Sexpr{round(effect_code_summary[3],2)*100}\% with 95\% probability (mean: \Sexpr{round(effect_code_summary[1],2)*100}\%).

The outcomes of the analysis are shown in Table \ref{tab:logistic}: the estimated probability of reproducing analysis of a paper for which only the data set is shared (intercept) is \Sexpr{round(quantile(intercept$intercept, 0.5),2)*100}\% with a Bayesian 95\% credible interval [\Sexpr{round(quantile(intercept$intercept, 0.025),2)*100}, \Sexpr{round(quantile(intercept$intercept, 0.975),2)*100}]\%. While both readme file and pre-registration seem to increase the probability of reproducing an analysis, these estimates also allow for 0 and negative effects. In contrast, the availability of code is associated with a great increase in probability of reproducing an analysis: the estimated increase due to the availability of code is \Sexpr{round(quantile(x$code, 0.5),2)*100}\% with 95\% CrI [\Sexpr{round(quantile(x$code, 0.025),2)*100}, \Sexpr{round(quantile(x$code, 0.975),2)*100}]\%.

<<modeling_summary, echo=FALSE,results='asis'>>=
m<-fixef(model_treatment)
m<-round(m,2)
print(xtable(m, comment=FALSE, caption = "Estimates of the log odds of replicating an analysis depending on the presence of analysis code, readme, and pre-registration.", label = "tab:logistic"), math.style.negative = TRUE, type="latex", caption.placement = "top")
@

\section{Discussion}

Although the reproducibility rate in our sample may seem discouraging, our estimate is not very different from those reported in other reproducibility attempts (Table~\ref{tab:reprotable}). \textcite{stodden2018empirical}, who analyzed the  reproducibility of papers published in Science after the data sharing policy took effect, report an estimated reproducibility of  26\%. \textcite{hardwicke2018data}, who assessed the reproducibility of papers published in \emph{Cognition}, estimated a reproducibility rate of 31\%  without author assistance, and an additional increase of 31\% when the authors of the original manuscript helped to reproduce the outcomes. Similarly, \textcite{hardwicke2021analytic} report a reproducibility rate of 36\% without author assistance, and an increase of 24\% with author assistance for the articles published in \emph{Psychological Science} that received an `open data badge'. Similar reproducibility rates were reported for political science \parencite{eubank2016lessons, stockemer2018data} and economics \parencite{chang2015economics}.

\begin{table}[!htbp] 
\caption{Summary of reproducibility rates in published investigations.} \label{tab:reprotable} \resizebox{\textwidth}{!}{% 
\begin{tabular}{lllll} 
\hline
Paper & Reproduced / total N & Percentage reproduced (95\% CI) \\
\hline
\textcite{stodden2018empirical} & 21 (extrapolated to 53) / 204$^{*}$ & 26 {[}20, 32{]}\% \\ 
\textcite{hardwicke2018data} & 11 / 35 & 31 {[}17, 51{]}\% \\ \textcite{hardwicke2021analytic} & 9 / 25 & 36 {[}20, 59{]}\% \\ \textcite{obels2020analysis} & 21 / 36 & 58 {[}39, 72{]}\% \\ \textcite{eubank2016lessons} & 4 / 24 & 17 {[}4, 29{]}\% \\ \textcite{stockemer2018data} & 32 / 70 & 46 {[}33, 56{]}\% \\ \textcite{chang2015economics} & 22 / 59 & 37 {[}24, 49{]}\% \\ \textcite{naudet2018data} & 14 / 17 & 82 {[}53, 94{]}\% \\\hline
\end{tabular}% 
}
\vspace{1ex}

\begin{small}
{\raggedright The asterisk (*) marks the fact that 56 articles were deemed reproducible by preliminary inspection. Of these, 22 were tested, and 21 were successfully reproduced. The reproducibility rate of 26\% is an extrapolation to the untested articles. \par}
\end{small}
\end{table}

<<bootstrapping_our_reproducibility, echo=FALSE>>=
meanfun <- function(data, i){
  d <- data[i, ]
  return(mean(d))   
}

# our estimate
papers_code <- data %>%
  filter(Data_accessible == 1 & Code == 1)

bo <- boot(papers_code[ , "Reproducible", drop = FALSE],
           statistic=meanfun, R=5000)
boot_lower <- boot.ci(bo, conf=0.95, type="bca")[4]$bca[4]
boot_upper <- boot.ci(bo, conf=0.95, type="bca")[4]$bca[5]

# Obels et al. estimate
## normal approximation has excessively tight intervals:
k<-21
n<-15+21
theta_hat<-k/n
obelsvar<-n*theta_hat*(1-theta_hat)
obelsSE<-sqrt(obelsvar/n)
obelslower<-n*theta_hat-2*obelsSE
obelsupper<-n*theta_hat+2*obelsSE

## Using bootstrapping:
Reproducible <- c(rep(0, 15), rep(1, 21))
obels <- data.frame(Reproducible)
bo_O <- boot(obels[ , "Reproducible", drop = FALSE],
           statistic=meanfun, R=5000)
boot_lower_O <- boot.ci(bo_O, conf=0.95, type="bca")[4]$bca[4]
boot_upper_O <- boot.ci(bo_O, conf=0.95, type="bca")[4]$bca[5]
@

Only one study assessing the reproducibility of publications in psychology reports a much higher estimate: \textcite{obels2020analysis} report a 58\% reproducibility rate. We believe that two factors may contribute to the difference in estimates: first, \citeauthor{obels2020analysis} assessed the reproducibility of registered reports \parencite[see][]{nosek2014registered}, whose authors are likely to be more motivated to engage in open science practices; and second, the authors attempted to reproduce only those papers that shared the analysis code. The authors report that 58\% (36 out of 62 papers that shared the data) shared the code, which is close to our estimate of 62\%. 

In our sample, the presence of analysis code increased  reproducibility, and the reproducibility rate for papers with code was very close to the one reported by \citeauthor{obels2020analysis}: our estimate was \Sexpr{round(mean(papers_code$Reproducible),2)*100}\% (95\% CI [\Sexpr{round(boot_lower,2)*100}, \Sexpr{round(boot_upper,2)*100}]\%), and the \citeauthor{obels2020analysis} estimate was 58\% (95\% CI [\Sexpr{round(boot_lower_O,2)*100}, \Sexpr{round(boot_upper_O,2)*100}]\%). Because \citeauthor{obels2020analysis} did not report confidence intervals, we  report bootstrapped intervals here for both our and the \citeauthor{obels2020analysis} estimates.   
                                                                                                                                                                                                                          
It is clear that reproducibility estimates from different studies are not directly comparable. Even the studies that report estimates similar to ours used different procedures and had different success criteria. For example, \citeauthor{hardwicke2018data} attempted to reproduce only one key finding in each manuscript, contacted authors and requested their help if the attempt to reproduce was unsuccessful, and labeled the attempt as successful when the discrepancy between the reported and the reproduced results was less than 10\%: all of these decisions are likely to increase the estimated reproducibility. Similarly, reproducing only those studies that share analysis code is likely to increase the estimate of reproducibility. 

Despite these limitations, we can still make a crude comparison between our reproducibility attempt and those of others in related fields. Even though we defined much stricter success criteria than either of the reproducibility attempts in psychology, we obtained a similar reproducibility estimate. Thus, the reproducibility rate in the papers considered here is no worse than in other fields.

The important questions that arise from our analysis are: What are the major barriers to reproducibility and what can we do as a field to improve reproducibility?  
  
Regarding the first question, we see four main problems: there are issues with the publicly released data, with the code, lack of documentation, and unclear terms of use.  These  issues  are listed below.

\begin{enumerate}
\item \textbf{Problems with the data}:

\begin{enumerate}
\item
Not all data that is reported in the paper is shared. For example:
\begin{enumerate}
\item Only aggregated data is shared for a study that analyzed unaggregated data.
\item Five experiments are reported in the paper, but there is data for only three experiments. Or four experiments are reported, but there is data only for three of them, etc. 
\item Some predictor variables / covariates used in the analysis are not in the data file. 
\item The data file that is used in the script is simply absent in the materials.
\item The data set used in the paper is an openly available corpus, but the novel annotation of the corpus done by the authors was not made available. Without that annotation, it is impossible to reproduce the analysis. 
\end{enumerate}
\end{enumerate}

\item 
\textbf{Problems relating to the analysis code}:
\begin{enumerate}
\item Not all the code necessary for reproducing the analysis is present.
\item Technical problems with the code:
\begin{enumerate}
\item The code produces an error message that we cannot debug.
\item The code is outdated and does not run any more (this is sometimes referred to as code rot). This happens because the environment in which the code was created changes; e.g., the syntax used for certain functions changes fundamentally, or some needed library is no longer publicly available, etc. 
\item The version of the  software package used (such as \texttt{lme4}) changes, leading to changes in the results; 
\item The code is written in proprietary software.
\end{enumerate}
\end{enumerate}

\item
\textbf{Problems with documentation}:
\begin{enumerate}
\item The shared data is difficult to interpret:
\begin{enumerate}
\item No readme file was available for the data set or opaque variable labels. In many cases, it is possible to reconstruct which column names correspond to the variables mentioned in the manuscript. In some cases, it is very hard or impossible.
\item Often the names of the data files do not correspond to how the experiments are numbered or presented in the manuscript, and there is no readme file that establishes the correspondence.
\end{enumerate}
\item The analysis is decribed in insufficient detail or the description contradicts the outcome of the analysis. Examples:
\begin{enumerate}
\item The manuscript claims that something was done that is not mirrored in the outcome of the analysis (e.g., the authors claim to have used X as a predictor in the model, but the table summarizing the outcome of the model does not include X as a predictor; the authors claim to have used a particular reference level for a factor variable, but the table summarizing the outcome shows that another factor level was used as a reference, etc.).
\item The authors state that they report the outcome of a stepwise elimination procedure for model selection, but forget to do so for one of the models, and without this information, the outcome cannot be reproduced.
\item Data trimming is often described very briefly and ambiguously, so that three different trimming procedures could fit the same description. Or data trimming is not mentioned at all. Often, we  could not reproduce the results following the analysis steps described in the manuscript, presumably because the authors did not report some preliminary manipulations with the data set -- trimming, centering and scaling of variables, etc. This could be easily avoided if analysis code was present.
\end{enumerate}
 
\end{enumerate}

\item 
\textbf{Unclear terms of use}: A common problem is that often no license is specified for the data and code. The absence of a license means that data and code could be under copyright protection and may not be usable by other researchers. It is not clear to us what the status is of data that have no license or terms of use specified for it. On the one hand, if code is released and no prohibition against further use is stipulated, the code and data should be usable by a third party. On the other hand, data and code are intellectual property, and so should be released with a license. A recent development is that funding agencies have started requiring data management plans or DMPs for experimental research (for an example, see https://osf.io/7c8sz/). DMPs usually require that one specify terms of use (e.g., a waiver) or a license. \textcite{dcclicense} is a useful guide to deciding on which license to use. 
\end{enumerate}

We turn now to the question of what steps can be taken to improve reproducibility.

\subsection{Steps that researchers can take to improve reproducibility}

An important point to appreciate is that reproducibility is inherently very difficult to achieve. As discussed above, the reasons for this can be quite mundane. The easiest way to fail to achieve reproducibility is, of course, to not provide the data or code that led to the published claims. For this reason, just providing the code along with the data will already improve the situation considerably (according to our estimates, providing code with data increases the reproducibility by \Sexpr{round(quantile(x$code, 0.5),2)*100}\% (95\% CrI [\Sexpr{round(quantile(x$code, 0.025),2)*100}, \Sexpr{round(quantile(x$code, 0.975),2)*100}]\%). So, even this one minimal change will have a big impact.


Below, we provide some suggestions for improving the reproducibility of analyses.  

\begin{enumerate}
\item \textbf{Release complete data in a usable form, with preprocessing code}: Make sure that all the data necessary to reproduce the analyses is shared. For example, if reporting that a covariate influences a particular conclusion (for example, the position of the trial in the experiment, or participants' age), this variable must be present in the data set. 

Share all the data collected: the data file need not consist of the raw output from the recording device; it can have been preprocessed. This is sometimes necessary because otherwise a very large amount of data would have to be stored online. However, all trimming and data transformations (including aggregation) should be performed in a pre-processing code script, and that script should be released with the data. In this context, the ``good enough'' computing practices advocated by \textcite{wilson2017good} are worth adopting.

One reason for releasing the raw data is that some  attributes of the data set may be valuable to other researchers. For example, the trial ID, the reading times on the last word in the sentence, or the average reading speed may be of no interest in the original paper, but could be highly relevant for researchers who study adaptation effects, sentence wrap-up effects, or average reading speed \parencite{brysbaert2019many}. Public data sets can be reused in ways that the authors haven't thought about \parencites[e.g., in meta-analyses,][]{mahowald2016meta,burki2020did}. 
\item \textbf{Use publicly accessible repositories for data sharing}: Share data on a publicly accessible server that can guarantee data being available for at least 10 years. It is generally not a good idea to store data on one's university homepage, because university homepages tend to frequently migrate and change urls. When researchers themselves move from one university to another, their old pages shut down and are no longer available. Providing data and code as part of the journal's supplementary section does not work in practice. We have only rarely succeeded in downloading materials from such links: either the link turns out to be dead, or nothing is available when one clicks on the link, or one needs a paid subscription to access the link.

Most papers that we surveyed shared their data set on Open Science Framework (https://osf.io/), Mendeley data (https://data.mendeley.com/), Zenodo (https://zenodo.org/) and some open institutional repositories. Other existing solutions include the Dataverse Project (https://dataverse.org/) and Github (https://github.com).

\item \textbf{Use non-proprietary data formats}: Sometimes data files are released as Word documents, Excel sheets, or even as pdf files. It is better to use use text or comma-separated-values (csv) files for data release. 

Storing data in Excel sheets is especially problematic because Excel makes some automatic decisions behind the scenes, for example, regarding the type of the input data. Researchers may not even be aware that the representation of their data has changed. This has already led to serious mistakes: for example, several human gene names are automatically converted to dates, which affected up to one-fifth of the surveyed papers \parencite{ziemann2016gene} and ultimately led to changes in the guidelines for genome naming \parencite{wain2002guidelines}; another example involves failure to report new COVID cases in the UK because an Excel spreadsheet containing lab results reached its maximum size, and failed to update for over a week\footnote{https://www.bbc.com/news/technology-54423988}.

\item \textbf{Provide documentation}: Write a description of your data set and variable names (usually stored in a file called ``readme'', ``codebook'' or ``data dictionary''). In addition, give variables and their values transparent and self-explanatory names. For example, if there are two conditions like subject relatives and object relatives, label the levels ``subj-rel'' and ``obj-rel'' or the like, instead of non-obvious labels like 1 and 2. Provide meta-data if your chosen storage server allows it: meta-data makes your data set more findable, which increases the chances that it will be reused and cited.

Related to documentation, the experiment item files (including fillers) should also be provided. These are usually not needed to ensuring reproducibility, but can be very valuable for carrying out replication attempts.

\item \textbf{Always share the data analysis code}: Share analysis code together with data. If the analysis code is not shared, it becomes very difficult and time-consuming, if not impossible, to trace back how the reported results were obtained. 

A nice illustration of the cost incurred by non-reproducible code is the case recently reported by \textcite{brewer2021discrepant}: two analyses of the same data set yielded completely different results, and ``it took the authors several days of email correspondence to determine where the differences were coming from'' (the main difference was the mean-centering of predictors in one of the analyses). Two studies of reproducibility in psychology report similar estimates: ``5â€“25 person hours'' \parencite[][, p. 12]{hardwicke2018data}, and ``between 1 and 30 (median = 7, interquartile range = 5) hours'' \parencite[][, p. 5]{hardwicke2021analytic} to reproduce one key result from a published paper. Scenarios like these can be avoided by simply providing reproducible analysis code.

A further complication is that the cases described above refer to the lucky scenario when the authors of the original manuscript are responsive and actively participate in the efforts to reproduce their analyses. Often this is not the case \parencite{wicherts2006poor, vanpaemel2015we}; in addition, author responsiveness and ability to provide information decrease with time after publication \parencite{vines2014availability, minocher2020reproducibility}, mostly due to email addresses that no longer work, and to lost or otherwise inaccessible data sets. For all of these reasons, it is highly desirable that all experimental materials, such as the data set, experimental items, and data analysis code, are shared in such a way that they can be used on a stand-alone basis, not dependent on the authors being able and willing to provide clarifications.

\item \textbf{Test the code and data release for usability}: After uploading data and code to a server, try downloading it from a different computer, preferably with a different operating system, and running the analysis. A common error we see in publicly available code is the specification of a full path name to a local directory to load data (e.g., C:\textbackslash Downloads\textbackslash MyData\textbackslash data), which a third party obviously cannot access. Relying on the relative (instead of absolute) file paths will work for everyone. 

A common outcome of testing whether the code runs on a different machine is that the uploaded analysis is not yet ready for release: some code or data files are not yet in the repository, some of the required software packages are not installed, etc. This can happen, for example, if the researchers rely on automatically saving some of the objects they created in a hidden .Rdata file, which is not available to a third-party user. After fixing the problems that arose as a result of this test, ask a colleague to download your materials and reproduce the analysis, to test whether a person who is not intimately familiar with the project can understand and run the analysis.

\item \textbf{Teach the next generation to use data management and computing skills}: A root cause for the problems that researchers face in producing reproducible research is that in psychology and psycholinguistics, historically there has been no tradition of teaching beginning researchers how to develop a research workflow that includes good quality code writing practices, data management, and documentation techniques. A good place to start would be to host a Data Carpentry (https://datacarpentry.org/) or a similar workshop on data management.

\end{enumerate}

Following these suggestions should eliminate the majority of the problems that we encountered. There are many recommendations and tutorials available for developing a better workflow. For a set of simple steps that increase reproducibility, refer to \textcite{obels2020analysis} and \textcite{wilson2017good}; for a gentle introduction to data sharing steps, such as choosing a repository, preparing data and code for sharing, structuring folders, see \textcite{klein2018practical} and https://www.ukrn.org/primers/; for the illustration of how the principles of high-reliability organizations, such as aviation and medicine, can be applied in the context of a scientific lab in psychology and related areas, see \textcite{rouder2019minimizing}; for a discussion of ``Findable, Accessible, Interoperable, and Reusable'' or FAIR  principles of data sharing and their implementation in practice, see \textcite{jacobsen2020fair}; and for a more technically involved set of recommendations for organizing a data analysis workflow, see \textcite{peikert2021reproducible}. 
 
Some good examples of code and data release that we found during our assessment are the following:

\begin{enumerate}
\item \textcite{boyce2020maze}:
https://github.com/vboyce/Maze.
\item \textcite{siew2021syllable}:
https://osf.io/adc2p/.
\item \textcite{nooteboom2020repairing}: 
https://osf.io/jahqe/.
\item \textcite{gunther2020immediate}:
https://osf.io/ftxjy/.   
\end{enumerate}

One important change that researchers might optionally consider adopting in their workflow is to use automated ways to incorporate numbers and figures in a manuscript. In our attempts to reproduce the manuscripts, we encountered copy-and-paste errors, such as two identical rows in a table summarizing the results of a linear model, or the loss of all minus signs in a large table. Copying and pasting numbers by hand is time-consuming and error-prone. Good automated alternatives for R include excellent packages \texttt{sjPlot} \parencite{sjPlot} and \texttt{apaTables} \parencite{stanley2018reproducible} producing publication-ready summary tables for Word and html, the \texttt{kableExtra} package producing tables for \LaTeX  \parencite{zhu2019kableextra}, and the \texttt{report} package for automatic generation of both tables and texts reporting the outcomes of statistical analyses \parencite{report}. R Markdown considerably simplifies the manuscript-preparation process; packages like \texttt{papaja} \parencite{aust2018papaja} allow the researcher to automatically format the paper in APA style, and to dynamically generate tables, figures, and numbers within the manuscript. Under this literate programming style \parencite{knuth1984literate}, the manuscript also serves as a complete documentation of the data analysis workflow. The present paper is written using a literate programming style.

\subsection{Potential concerns to developing a reproducible workflow}

Here, we pre-emptively address some concerns that researchers might have to developing a reproducible workflow. 

A common concern is that writing code and documentation in a publicly accessible way will be very time-consuming and will take time away from core scientific work. One may also lose time if one has to acquire new data management and coding skills (e.g., learning R Markdown for document preparation). 

There is no denying that preparing the data set and analysis code for archiving is indeed costly \parencite[22\% of psychology researchers share this concern, see][]{borghi2021data}. Three considerations may help here. First, if the researcher adopts a reproducible workflow, the initial time investment is likely to be fully repaid during the active phase of working on a project. For example, the entire analysis for a slightly changed data file can be carried out by simply re-running the analysis file. If one adopts a literate programming style, all the tables, figures, and in-text numbers are generated automatically. Second, when preparing the code for public release, the researcher may catch errors that may otherwise have gone unnoticed. Finally, if, as required by many journals, code and data are released during the review process, mistakes can be caught before the paper is published. For example, in one case \parencite{jager2020interference}, a reviewer (Brian Dillon) examined the code and found an error in the code while reviewing the paper; we corrected the error in the revision. If we hadn't provided the code and data, this error could have gone undetected.



Another potential concern is: will anyone ever look at the published data and code? If not, why invest time and effort into releasing these? It is indeed impossible to know whether other researchers will engage with one's data, but if the data and code are not shared, it is guaranteed that nobody will use it, because nobody can. The easier a data set is to access publicly, the higher the chance that it will be reused. In addition, as mentioned earlier, some aspects of the data set may turn out to be useful for answering new research questions. 

Several surveys of data sharing practices show that less experienced researchers are hesitant to share data and analysis code out of fear of public shaming, loss of reputation, etc., if any errors are found \parencite{meteyard2020best, soeharjono2021reported}. It is possible that errors may be found, but this does not necessarily lead to an adverse effect on one's career \parencite{ebersole2016scientists}. While we cannot speak for all researchers, in our experience, usually those who are good at spotting mistakes are good at it precisely because they found a lot of mistakes in their own code, and therefore tend to be more understanding towards mistakes in other people's code. The reality is that everyone makes mistakes; it is not a personal failure on a researcher's part when a mistake is discovered. 

One strategy for overcoming the fear of having an error detected publicly is to ask a trusted colleague for feedback before releasing the data and code; this is no different than asking for feedback on a to-be-submitted manuscript. One can go one step further and organize a code review group within one's lab, similarly to writing groups that exist in many research groups. Finally, while mistakes in the code are traditionally associated with compromising the main theoretical claim of the work, this is by no means the most common outcome. In our attempts to reproduce the published analyses, we repeatedly found effects that supported the main theoretical claim of the manuscript but were not reported by the authors. In other words, publishing and reviewing code can strengthen the theoretical claims of the paper instead of casting doubt on them.

In sum, the practices suggested above do come with a price, but we believe that this is a price worth paying both for the benefit of individual researchers and for the global benefit of the scientific community. Reproducibility is important and worth aiming for because it increases trust in science, and enables scientists to feel more confident about what they learned from previous work. This is a very valuable outcome for everyone, and the extra effort required to approach reproducibility will help all researchers. 

\subsection{Changes that journals can make}

Journals are already actively instituting policies that are fostering positive change. Below, we list some further changes that journals can make. 

\begin{enumerate}
\item Make it obligatory to share analysis code along with the data, unless there are compelling reasons not to do so.
\item Stipulate a clear section in the paper in which the link to the materials should always appear. Currently, the links can be found all across the paper text, from the first mention of the data set in the Methods section to Acknowledgements. In several papers the official research data statement was ``Data not available / Data will be made available on request'', but the link to the data set was in fact found in some place in the paper.
\item Have an editorial assistant check the technical requirements upon submission. Specifically, the assistant should check that:
\begin{enumerate}
\item the link indeed leads to the data and code
\item a license is specified that allows some kind of reuse
\end{enumerate}
\item Provide in-house assessment of reproducibility \parencite[for a discussion and analysis of the efficiency of such measures, see ][]{eubank2016lessons, sakaluk2014analytic}. Many journals, like \textit{Language} and \textit{Glossa Psycholinguistics}, are planning to have or already have statistical consultants on their editorial board.
\item Engage reviewers who are part of the The Peer Reviewers' Openness (PRO) Initiative \parencite{morey2016peer} and who therefore will further ensure that open science practices are implemented.
\item Carry out periodic evaluations like the present one to evaluate whether the policies are working.
\end{enumerate}


\section{Conclusion}
We assessed the reproducibility of 57 papers published in Journal of Memory and Language after a mandatory data sharing policy was adopted and were able to reproduce \Sexpr{sbst %>% filter (Reproducible == "yes") %>% nrow()} (\Sexpr{100 * round(sbst %>% filter (Reproducible == "yes") %>% nrow() / sbst  %>% nrow(),2)}\%). Most cases of irreproducibility could have been avoided if the authors had attempted to reproduce their own analyses using only the data and code they shared. We suggest that such a self-administered reproducibility check, similar to the proofreading of a manuscript, should be a part of normal research workflow. 

Many problems that we listed block the very beginning of the data analysis pipeline (missing data, missing variable descriptions, unclear data selection and trimming procedures), so it is possible that our suggestions, while helping to solve these problems, would expose other problems later in the pipeline that would call for different solutions and quality control procedures. For example, we only considered the reproducibility of the results under the analysis chosen by the authors of the original manuscript. A more challenging assessment of reproducibility would be to test if the key results hold under different, equally well-motivated analyses \parencite[for an example of such assessment, see][]{silberzahn2018many}. The results that could pass such a test, and be independently replicated, will also serve to contribute to increased reliability in science. 

\section{Supplementary materials}

The code and anonymized data for regenerating this paper are available from https://osf.io/3bzu8/. For confidentiality reasons, we do not release the analyses that we carried out, but the editor and reviewers of this article will have access to our analyses.

\section{Acknowledgements}

This work was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation), project number 317633480, SFB 1287 (2021-2025), Project Q, PIs: Shravan Vasishth and Ralf Engbert. We are grateful to Dario Paape for comments on a draft.

\printbibliography

\end{document}

