\documentclass[man,floatsintext]{apa6}

\usepackage{doi}
\usepackage{longtable}
\usepackage[american]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[backend=biber,useprefix=true,style=apa,url=false,doi=false,sorting=nyt,eprint=false]{biblatex}

\usepackage{fancyvrb}

\usepackage{newfloat}
\DeclareFloatingEnvironment[
%    fileext=los,
%    listname=List of Schemes,
%    name=Listing,
%    placement=!htbp,
%    within=section,
]{listing}

\usepackage{lscape} % landscape table
\usepackage{longtable}
\usepackage{threeparttablex}
\usepackage{booktabs}
\usepackage{multirow} % multirows in tables 
\usepackage{bigdelim} % curly braces in table
\usepackage{xcolor,colortbl}

\usepackage{mathtools}
\usepackage[inline]{enumitem}
\makeatletter
 
\newcommand{\explain}[2]{\underset{\mathclap{\overset{\uparrow}{#2}}}{#1}}
\newcommand{\explainup}[2]{\overset{\mathclap{\underset{\downarrow}{#2}}}{#1}}
 
\makeatother

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{microtype}
\usepackage[american]{babel}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage{setspace}
\usepackage{geometry}
\usepackage{lineno,clipboard}
\newclipboard{reviews}
\openclipboard{reviews}

\newcommand{\revised}[1]{{\color{black}{#1}}}

\usepackage{tikz}
\usetikzlibrary{arrows.meta}
\tikzset{%
  >={Latex[width=2mm,length=2mm]},
  % Specifications for style of nodes:
            base/.style = {rectangle, rounded corners, draw=black,
                           minimum width=4cm, minimum height=1cm,
                           text centered, font=\sffamily},
  activityStarts/.style = {base, fill=blue!30},
       startstop/.style = {base, fill=red!30},
    activityRuns/.style = {base, fill=green!30},
         process/.style = {base, minimum width=2.5cm, fill=orange!15,
                           font=\ttfamily},
}

\usepackage{gb4e}


\DeclareLanguageMapping{american}{american-apa}
\addbibresource{LYVReproducibility2022.bib}


\leftheader{Laurinavichyute, Yadav, and Vasishth}
\title{Share the code, not just the data: A case study of the reproducibility of JML articles published under the open data policy}
\shorttitle{Share the code, not just the data}

\threeauthors{Anna Laurinavichyute}{Himanshu Yadav}{Shravan Vasishth}

\threeaffiliations{Department of Linguistics, University of Potsdam, Potsdam, Germany}{Department of Linguistics, University of Potsdam, Potsdam, Germany}{Department of Linguistics, University of Potsdam, Potsdam, Germany}

\authornote{Please send correspondence to anna.laurinavichyute@uni-potsdam.de.}
\note{\today}

\journal{Submitted to the Journal of Memory and Language} 
\volume{-} 

\keywords{open data; reproducible statistical analyses; reproducibility; open science; meta-research; journal policy}

<<setup,include=FALSE,cache=FALSE,echo=FALSE>>=
knitr::opts_chunk$set(echo = FALSE, purl=TRUE)
library('knitr')
library('coda') 
library('plyr') 
library('ggplot2') 
library('xtable')
library('dplyr')
library('SIN')
library('tidyverse') 
library('tidybayes')
library('rstantools')
library('brms')
library('riverplot') 
library('boot')
@

<<loaddata, include=FALSE>>=
data <- read.delim("../data/overview_numeric_anon.csv", sep=";", 
                   header=TRUE, na.strings=c(""))%>%
  select(-Processed, -Assigned) 

# Open data policy subset
sbst <- data %>%
  filter(Group == "Open data policy")

papers_with_data <- sbst %>% filter(Data_accessible == "yes") 
papers_with_data_no_code <- sbst %>% filter(Data_accessible == "yes", Code == "no") 
papers_with_code <- papers_with_data %>% filter(Code == "yes") 
papers_with_full_data <- sbst %>% filter(Data_accessible == "yes" & Full_dataset == "yes")


# Papers published before open data policy
pre_policy <- data %>%
  filter(Group == "Before open data policy")

papers_with_data_pp <- pre_policy %>% filter(Data_accessible == "yes") 
papers_with_data_no_code_pp <- pre_policy %>% filter(Data_accessible == "yes", Code == "no") 
papers_with_code_pp <- papers_with_data_pp %>% filter(Code == "yes") 
papers_with_full_data_pp <- pre_policy %>% filter(Data_accessible == "yes" & Full_dataset == "yes")
@



\abstract{In 2019 the Journal of Memory and Language instituted an open data and code policy; this policy requires that, as a rule, code and data be released at the latest upon publication. How effective is this policy? \revised{We compared 59 papers published before, and 59 papers published after, the policy took effect. After the policy was in place, the rate of data sharing increased by more than 50\%. We further looked at whether papers published under the open data policy were} reproducible, in the sense that the published results should be possible to regenerate given the data, and given the code, when code was provided. \Copy{abstract}{\revised{For \Sexpr{sbst %>% filter(Data_accessible == "no") %>% nrow()} out of the 59 papers,} data sets were inaccessible. \revised{The reproducibility rate ranged from 34\% to 56\%, depending on the reproducibility criteria. The strongest predictor of whether an attempt to reproduce would be successful is the presence of the analysis code: it increases the probability of reproducing reported results by almost 40\%. We propose two simple steps that can increase the reproducibility of published papers: share the analysis code, and attempt to reproduce one's own analysis using only the shared materials.}}} 

\DeclareUnicodeCharacter{0301}{\'{e}}
 
\begin{document}

\maketitle


\section{Introduction}

Being able to build on existing knowledge is key to scientific progress. A prerequisite for such incremental knowledge gain is that \Copy{reliability}{existing research be reliable, in the sense that we can be relatively confident that the claimed findings in a paper reflect something that is true about the phenomenon being investigated.}  \label{reliabilitylabel} The replicability of published claims is a key component of reliability \parencite{munafo2017manifesto, NAP25303}. Here, replicability can be broadly understood to mean that one can obtain the same or similar conclusions as in a published result when one repeats the same experiment with new participants.\footnote{\Copy{definitionreplicability}{\revised{By ``same or similar'' we are not referring to statistical significance/non-significance but rather to the broad consistency of estimates across replications \parencite[for detailed discussion about consistency of observed patterns in the psycholinguistic context, see][]{VasishthGelman2021}}}.}

In recent years, the replicability of apparently well-established results in psychology has been called into question \parencite[e.g.,][]{anderson2015estimating}. Since then, within psychology and psycholinguistics several failed replication attempts of well-known claims have been reported \parencites[e.g.,][]{klein2014investigating, hagger2016multilab, klein2018many, nieuwland2018large, stack2018failure, jager2020interference, nieuwland2020anticipating, vasishth2018statistical, mertzen2020cross}. Such failures to replicate raise important questions about the extent of the non-replicability problem in psycholinguistics and related areas. Partly in response to this concern, a special issue of the \emph{Journal of Memory and Language} focused on the replication of influential findings in memory and language research. This move from the journal is a clear signal to the field that replication has an important role to play in scientific progress. 

Replicability is widely regarded as an important aspect of assessing the reliability of a particular finding. However, if one fails to replicate a study, it is often difficult to work out exactly why the failure to replicate happened. This is because the failure may be driven by many factors that are external to the research question under investigation: differences in the population and/or language studied, the natural variability in the dependent variable, lab settings, equipment, and protocols can come together to lead to very different outcomes. \Copy{replicabilitycaveat}{\revised{Indeed, although large effects  are generally easy to replicate \parencite[an example is certain garden path constructions,][]{PaapeVasishthBSPR2022}, when it comes to studying subtle and highly variable aspects of human behavior, replicability may well be an unattainable goal. This inherent variability of effects is why statisticians like Andrew Gelman often emphasize the need to embrace variation and accept uncertainty (\url{https://youtu.be/E8uzPjg1mR8}).}}

\Copy{control}{\revised{Thus, the replicability of any particular finding could be beyond the control of the researcher because of the type of phenomenon being studied or the inherent variability in the behavioral response. However, there is another important aspect of reliability that is directly under the researcher's control.}} This fundamental aspect is the reproducibility of published claims. Reproducibility refers to the ability to regenerate the key summary statistics (e.g., means, standard errors, t-, F-, or p-values) reported in a paper using the data (and code) provided with the paper \parencites{nuijten2018verify, lebel2018unified, stodden2018empirical}. Given some fixed data, if the key summary statistics that the statistical inference is based on are not reproducible, it is unclear what a replication attempt should even aim to replicate.

On the face of it, reproducibility might seem to be an easily attainable goal: is one not bound to obtain the same results on the original data set if one simply re-runs the data analysis code again? \Copy{notagiven}{In recent years, researchers in psychology and other areas have come to learn that success in reproducing the original results is not a given, \revised{mainly due to the absence of data and code}}. Researchers in different fields have been investigating the reproducibility of published claims. Some examples are \revised{psychology \parencite{nuijten2016prevalence}}, political science \parencite{stockemer2018data}, economics \parencite{chang2015economics}, biomedical science \parencite{naudet2018data}, machine learning \parencite{raff2019step}, and hydrology \parencite{stagge2019assessing}. Across all these studies, the reproducibility rate has been reported to be around 30\%, ranging from 17\% in political science \parencite{eubank2016lessons} to 37\% in economics \parencite{chang2015economics}, with some exceptions (58\% for registered reports in psychology, \cite{obels2020analysis}; \revised{70\% for articles whose authors shared data upon request, \cite{artner2020reproducibility}; 55\% for meta-analyses in psychology, \cite{maassen2020reproducibility}}; 82\% in biomedicine, \cite{naudet2018data}). In recognition of this irreproducibility crisis, the journal \emph{Cortex} has introduced a new article type, the verification report. The sole purpose of this article type is to repeat the original analyses or report new analyses of the original data set \parencite{chambers2020verification}.

One of the most basic barriers to reproducibility is data unavailability: if the original data set is not openly available \parencite[which is often the case, see][]{wicherts2006poor, vanpaemel2015we}, independent analysis cannot be performed. In response to this challenge, many journals in psychology and linguistics have adopted a mandatory data sharing policy. Examples are \emph{Cognition}, \emph{Cortex}, \emph{Collabra:Psychology}, \emph{Open Mind}, \emph{Glossa Psycholinguistics}, \emph{Journal of Cognition}, and \emph{Computational Brain and Behavior}. More than a thousand  journals from a wide range of fields are signatories to the Transparency and Openness Promotion (TOP) guidelines from the Center for Open Science (\url{https://www.cos.io/initiatives/top-guidelines}).

The introduction of such guidelines and policies has been effective in increasing the proportion of open data sets accompanying articles \parencite{nuijten2017journal, hardwicke2018data}. However, although ensuring data availability is a necessary condition for reproducibility, it may not be sufficient: \textcite{towse2021opening} analyzed the quality of open data sets across psychological journals. They report that 51\% of the surveyed data sets were not sufficiently documented or did not contain all the data needed to reproduce the reported analyses, and 68\% were archived in a way that limits reusability. \Copy{nonreusability}{Examples of non-reusability were data that were not machine-readable, aggregated, or missing \parencite[for a similar point, see also][]{hardwicke2021analytic}}.

\Copy{commissioned}{The \emph{Journal of Memory and Language} was among the first linguistics-oriented journals to adopt, in 2019, the mandatory data sharing policy, thus making the first step on the path to ensuring reproducibility of published findings \parencite{gerrig2019new}. \revised{To assess the impact of this policy, in May 2020, the editor-in-chief of the \emph{Journal of Memory and Language} (Professor Kathleen Rastle) commissioned an independent evaluation of whether the rate of data sharing has increased and whether the shared data is sufficient to reproduce the published results}.} The aim of the present work is to evaluate the data sharing policy and to investigate the reproducibility of papers published in the \emph{Journal of Memory and Language} after this policy was instituted.

\section{Evaluation of the reproducibility of JML articles (2019--2021)}

\revised{\subsubsection{Open data policy} 

\Copy{confronting}{When submitting a manuscript to the Journal of Memory and Language, on the first page of the submission form, the authors are confronted with the open data policy: \begin{quotation}
``We require articles published in the Journal to make publicly available any stimuli, data, analysis code, and computational models associated with the research. Because these materials are an important part of the research that the Journal is reviewing, we require authors to provide a (private) link to them at the point of submission. Please include this link on the title page of your manuscript. Manuscripts that do not include access to these materials will usually be returned to authors. \linebreak

Further information about our data sharing policy is available \href{https://www.sciencedirect.com/science/article/pii/S0749596X18300883}{here}.''
\end{quotation}}
}

\subsubsection{Material}

The editor-in-chief gave us the titles of papers reporting quantitative experiments that were published \revised{before (N=59) and} after (N=59) data sharing was made mandatory in JML in 2019. 

The list of papers published before the open data policy took effect: \textcite{veldre2018beyond, stefanidi2018free, zhang2018speech, wedel2018phonetic, yim2018evidence, jones2018does, zawadzka2018remind, singh2018working, fukumura2018ordering, slioussar2018forms, kowialiewski2018non, wen2018limitations, drummer2018cataphoric, isarida2018influences, wang2018nature, scott2018language, sahakyan2018divided, hopper2018learning, cunnings2018retrieval, healey2018temporal, arnold2018linguistic, seedorff2018detecting, karimi2018electrophysiological, chan2018testing, chubala2018does, mckoon2018adults, fisher2018patterns, miyoshi2018comparing, james2018individual, mohanty2018mitigating, keung2018variable, akan2018testing, de2018voluntary, uner2018encoding, vaughn2018listener, veldre2018does, osth2018list, hsiao2018semantic, jou2018does, frazier2018topic, vasishth2018statistical, van2018linguistic, galati2019social, miller2019individual, susser2019exploring, fritz2019information, seabrooke2019learning, malejka2019exploring, wilson2019making, thalmann2019revisiting, gathercole2019working, nooteboom2019temporal, do2019subjecthood, paap2019encapsulation, van2019scales, van2018infants, nicenboim2018models, deliens2018context}.

The list of papers published after the open data policy took effect: \textcite{samuel2020psycholinguists, skrzypulec2020nonlinear, lelonkiewicz2020morphemes, lange2019linking, meteyard2020best, chan2020does, brainerd2020norming, avetisyan2020does, ahn2020retrieval, troyer2020catch, hollis2020delineating, johns2020production, brysbaert2019many, nooteboom2020repairing, bangerter2020lexical, gagne2020buttercup, mckinley2020role, brothers2021word, diez2020linguistic, brainerd2020explaining, brandt2020computational, villani2021sensorimotor, fox2020accounting, schubert2020reading, collins2020minerva, yang2020origins, snefjella2020emotion, isarida2020video, siew2021syllable, hwang2019cumulative, brainerd2019super, osth2020global, chetail2020graphemic, gunther2020immediate, fellman2020role, tirso2020taking, bristol2020epistemic, floccia2020translation, corps2020top, burki2020did, kaula2020priming, snell2020story, humphreys2020semantic, gunther2020semantic, li2020does, saito2020domain, fujita2020reanalysis, boyce2020maze, falandays2020long, ambrus2020less, hesse2020scalar, garnham2020anticipating, liang2021initial, reifegerste2020effects, lauro2020bilingual, tsuboi2020rethinking, jager2020interference, siegelman2020individual, brewer2021discrepant, lange2019linking, meteyard2020best}.

\subsection{Methods}

We downloaded any available material\footnote{\revised{For one paper in the post-policy list and nine papers in the pre-policy list, materials were published on the journal website; they were forwarded to us by the editor-in-chief.}} \revised{published with every paper. Data were labeled as accessible} when at least some subset (but not necessarily all) of the data described in the paper had been made available. We additionally annotated whether the code for performing the analysis was present, the analysis was preregistered, and whether data was accompanied by a readme file or the like explaining what variable names and their values in the data file stand for. These factors could potentially affect reproducibility: the analysis code documents all the analysis steps; the readme file identifies the explanatory and dependent variables; and a preregistration may be associated with a more reproducible analysis code (for example, \textcite{obels2020analysis} reported a relatively high reproducibility rate, 58\%, for registered reports in psychology).

\Copy{methods}{\revised{For every paper published under the open data policy,} we attempted to reproduce the analysis using the description in the paper itself, and the analysis code, if this was provided. \revised{When a reported value could not be reproduced, alternative computations compatible with the analysis description were attempted. For example, when the reported condition mean could not be reproduced with data aggregated within participants, we computed the mean without the aggregation step. When the difference between the reproduced and the reported values was greater than 10\% for at least 20 reported values, the attempt to reproduce was terminated. If data for some of the reported experiments or analyses was missing, we attempted to reproduce the remaining results. \Copy{noclarifications}{In contrast to some reproducibility assessments \parencite{hardwicke2018data, hardwicke2021analytic}, we did not contact the authors and ask for clarifications if the results could not be reproduced. This decision follows from the goal of computational reproducibility: to obtain the same results using available data and procedures. If not all analysis steps were described in sufficient detail, then the study is not reproducible for the reader, even if these analysis steps are fully documented for private use.}}\label{methodslabel} \label{noclarificationslabel}} \revised{The outcomes of the reproducibility attempts were evaluated under several criteria, ranging from a strict to several increasingly relaxed ones.}
   
\emph{Strict criterion:} Papers were labeled as reproducible if all the reported analyses could be reproduced exactly, including reported means (except for the rounding errors and the edges of Bayesian credible intervals, where minor fluctuations are expected). If we could not reproduce the exact numbers, even when all the results reported as significant remained significant, the paper was considered to be not reproducible. The reason we focus on the reproducibility of the summary statistics and not whether an effect was significant or not is that statistical significance per se is not a very informative result, unless the power properties of the experimental design are also known \parencites{gelmancarlin, JaegerEngelmannVasishth2017, vasishth2018statistical, jager2020interference, VasishthGelman2021}.
   
The exact effect estimates from an analysis could be not reproducible for a number of reasons, including updates to the software used to run the analysis, using a updated version of the optimizer for fitting a linear mixed model, etc. \revised{Note, however, that summary statistics such as means and standard deviations, do not depend on the software.} Obviously, the fact that a study was not reproducible under the strict criterion does not necessarily say anything about the quality of scientific evidence presented in the study, but rather about the quality of data and analysis presentation. 

\Copy{newcriteria}{\revised{While being straightforward and easy to evaluate, the strict reproducibility criterion may be seen as too mechanical: it does not necessarily correspond to researchers' intuition about reliable results. Most researchers would probably still consider results to be reliable even if several reproduced values slightly deviated from the reported ones. For this reason, we introduced a set of more relaxed criteria that may better correspond to the intuitive notion of reproducibility. 

\emph{Relaxed criteria:} If there are less than \emph{K} cases where the reproduced value differs from the reported value by a margin of more than 10\%, the study is considered reproducible under a relaxed criterion. Discrepancies smaller than 10\% of the reported value are disregarded. This criterion is evaluated at several \emph{K}s: 1, 5, 10, and 20. For example, we find that we cannot reproduce four values by a margin of (more than) 10\%: in four cases, the reproduced value differs from the reported value by a margin of over 10\%. In this situation, we say that the study is reproducible under K=5 criterion, but not under K=1 criterion.  

The above range of reproducibility criteria (K: 1 to 20) was chosen based on the following rationale. The lower threshold, a single major discrepancy blocking reproducibility, corresponds to the criterion used in the reproducibility assessments by \textcite{hardwicke2018data, hardwicke2021analytic}. The upper threshold corresponds to major discrepancies in up to 10\% of values reported in a paper (most papers report more than 200 values). We suggest this as the upper threshold for discrepancies in papers that could still be considered reproducible in a broad sense. The ``10\% criterion'' subsumes all possible discrepancies: neither p-values nor missing data receive special treatment. If data for some experiment is missing, and more than 20 values are reported for this experiment, the paper is labeled as irreproducible.

We do not evaluate whether the main claims of the study hold: identifying which results correspond to the main claims is a nuanced decision that is not always within our expertise. For each study, the reader can make their own informed decision based on the published attempt to reproduce the results.}\label{newcriterialabel}

}
\subsection{Descriptive statistics}

\begin{figure}[!htbp]
\centering
<<summaryfig,echo=FALSE,fig.height=2.3>>=
data_plot <- data %>%
  rename("Data accessible" = "Data_accessible") %>%
  select(`Data accessible`, Preregistered, Code,
         Readme, Group)

data_long <- gather(data=data_plot, key=condition, value=y,
                    `Data accessible`:Readme,
                    factor_key=TRUE)
data_long$y<-factor(data_long$y)

data_long %>%
  group_by(Group) %>%
  count(condition,y) %>%
  pivot_wider(names_from = y, values_from = n)%>%
  reshape2::melt(id = c("Group", "condition")) %>%
  ggplot(aes(x = condition, y = value,
             fill = variable, label = value)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ Group) + 
  geom_text(size=4, position = position_stack(vjust=0.5)) +
  coord_flip() + theme_bw() +
  scale_fill_grey(start = 0.9, end = 0.75,
                  guide=guide_legend(reverse=TRUE)) +
  xlab("") + ylab("") +
  theme(legend.title = element_blank(),
        text = element_text(size =15))
@
\caption{A summary showing the number of papers which had a readme file or the like that provided some documentation, for which the code was available, which were preregistered, and for which the data were accessible.} \label{fig:summaryplot}
\end{figure}


The key properties of the data and code release that may affect reproducibility are summarized in Figure \ref{fig:summaryplot}. Open data policy has increased the rate of data sharing by more than 50\%: \revised{\Sexpr{papers_with_data_pp %>% nrow()} (\Sexpr{100 * round(papers_with_data_pp %>% nrow() / pre_policy %>% nrow(), 2)}\%) papers in the pre-policy group vs. \Sexpr{papers_with_data %>% nrow()} (\Sexpr{100 * round(papers_with_data %>% nrow() / sbst %>% nrow(), 2)}\%) papers in the post-policy group} had downloadable data sets. \revised{While data sharing in the pre-policy group was voluntary, in the post-policy group, the authors were required to make the data openly available. Still,} the data were inaccessible for \Sexpr{sbst %>% filter(Data_accessible == "no") %>% nrow()} papers. 
The reasons for data unavailability were the following:

\begin{itemize} 
\item \revised{Four papers had no link to data/computational model introduced in the paper.} \label{missingdata}
\item  \revised{For three papers,} the data cannot be retrieved from the link provided. The reasons were diverse: the page does not exist \revised{(N=1), the link leads to a university repository that requires a log-in using the university account (N=1), supplementary materials contain a description of additional analyses instead of data (N=1)}. 
\item
Two papers stated that ``data will be available upon request''. For one request, we received data for two experiments out of four, along with a promise to provide the rest of the data if we request it; \revised{however, after we requested the rest of the data, the author refused to share the remaining data because they did not know what the data set is being used for, and because the data release would be too time-consuming}. The other request was met with a question about the planned use for the requested data. In both cases, we did not reveal that we were carrying out a reproducibility check, and marked these data sets as partially/fully inaccessible, since the journal policy states that data should be made available unconditionally. 
\end{itemize}

\revised{In the pre-policy group, of \Sexpr{papers_with_data_pp %>% nrow()} papers with accessible data, \Sexpr{papers_with_code_pp %>% nrow()} (\Sexpr{100 * round(papers_with_code_pp %>% nrow() / papers_with_data_pp %>% nrow(), 2)}\%) also shared analysis code. Most papers (\Sexpr{papers_with_full_data_pp %>% nrow()} of 20, \Sexpr{100 * round(papers_with_full_data_pp %>% nrow() / papers_with_data_pp %>% nrow(), 2)}\%) shared all the data necessary to reproduce the reported analyses, that is, all independent and dependent variables reported in the manuscript were present in the data file.
} 

\revised{In the post-policy group, of \Sexpr{papers_with_data %>% nrow()} papers with accessible data,
\Sexpr{papers_with_code %>% nrow()} (\Sexpr{100 * round(papers_with_code %>% nrow() / papers_with_data %>% nrow(), 2)}\%) also shared analysis code. Only \Sexpr{papers_with_full_data %>% nrow()} (\Sexpr{100 * round(papers_with_full_data %>% nrow() / papers_with_data %>% nrow(), 2)}\%) shared all the data necessary to reproduce the reported analyses.}

\subsection{Results}

\emph{Strict criterion}. Of \Sexpr{papers_with_data_no_code %>% nrow()} papers that shared data but not analysis code, only \Sexpr{papers_with_data_no_code %>% filter(Reproducible == "yes") %>% nrow()} (\Sexpr{100 * round(papers_with_data_no_code %>% filter(Reproducible == "yes") %>% nrow() / papers_with_data_no_code %>% nrow(), 2)}\%) could be fully reproduced. Of \Sexpr{papers_with_code %>% nrow()} papers that shared both data and analysis code, \Sexpr{papers_with_code %>% filter(Reproducible == "yes") %>% nrow()} papers (\Sexpr{100 * round(papers_with_code %>% filter(Reproducible == "yes") %>% nrow() / papers_with_code %>% nrow(), 2)}\%) could be fully reproduced. Overall, \Sexpr{sbst %>% filter (Reproducible == "yes") %>% nrow()} papers could be fully reproduced, which constitute \Sexpr{100 * round(papers_with_data %>% filter (Reproducible == "yes") %>% nrow() / papers_with_data %>% nrow(),2)}\% of all papers that had accessible data, and \Sexpr{100 * round(sbst %>% filter (Reproducible == "yes") %>% nrow() / sbst  %>% nrow(),2)}\% of the 59 surveyed papers. Figure \ref{fig:reproducibility}A summarizes the breakdown of reproducible papers depending on the presence of analysis code.

\revised{\emph{Relaxed criteria}. The outcomes of the reproducibility assessment under increasingly relaxed criteria are summarized in Table \ref{tab:relaxed}. For the remainder of this paper, we will focus on the most lenient criterion that tolerates up to 20 major discrepancies between the reported and the reproduced values. Under this criterion, the number of reproducible papers that shared data but not analysis code increased to \Sexpr{papers_with_data_no_code %>% filter(Relaxed_reproducible_20 == "yes") %>% nrow()} of \Sexpr{papers_with_data_no_code %>% nrow()} (\Sexpr{100 * round(papers_with_data_no_code %>% filter(Relaxed_reproducible_20 == "yes") %>% nrow() / papers_with_data_no_code %>% nrow(), 2)}\%). Of \Sexpr{papers_with_code %>% nrow()} papers that shared both data and analysis code, \Sexpr{papers_with_code %>% filter(Relaxed_reproducible_20 == "yes") %>% nrow()} papers (\Sexpr{100 * round(papers_with_code %>% filter(Relaxed_reproducible_20 == "yes") %>% nrow() / papers_with_code %>% nrow(), 2)}\%) could be reproduced. Overall, \Sexpr{sbst %>% filter (Relaxed_reproducible_20 == "yes") %>% nrow()} papers could be fully reproduced, which constitute \Sexpr{100 * round(papers_with_data %>% filter (Relaxed_reproducible_20 == "yes") %>% nrow() / papers_with_data %>% nrow(),2)}\% of all papers that had accessible data, and \Sexpr{100 * round(sbst %>% filter(Relaxed_reproducible_20 == "yes") %>% nrow() / sbst  %>% nrow(),2)}\% of the 59 surveyed papers. Figure \ref{fig:reproducibility}B summarizes the breakdown of reproducible papers depending on the presence of analysis code.}

\begin{table}[]
\caption{The number of papers classified as reproducible according to different reproducibility criteria.}
\label{tab:relaxed}
\begin{tabular}{lll}
\hline
                                 & \multicolumn{2}{c}{Reproducible papers}  \\
                                 & Without code (of 19) & With code (of 32) \\ \hline
Strict reproducibility criterion & \Sexpr{papers_with_data_no_code %>% filter(Reproducible == "yes") %>% nrow()}                    & \Sexpr{papers_with_code %>% filter(Reproducible == "yes") %>% nrow()}                \\
Only minor discrepancies        & \Sexpr{papers_with_data_no_code %>% filter(Relaxed_reproducible_1 == "yes") %>% nrow()}                    & \Sexpr{papers_with_code %>% filter(Relaxed_reproducible_1 == "yes") %>% nrow()}                \\
Up to 5 major discrepancies      & \Sexpr{papers_with_data_no_code %>% filter(Relaxed_reproducible_5 == "yes") %>% nrow()}                    & \Sexpr{papers_with_code %>% filter(Relaxed_reproducible_5 == "yes") %>% nrow()}                \\
Up to 10 major discrepancies     & \Sexpr{papers_with_data_no_code %>% filter(Relaxed_reproducible_10 == "yes") %>% nrow()}                    & \Sexpr{papers_with_code %>% filter(Relaxed_reproducible_10 == "yes") %>% nrow()}                \\
Up to 20 major discrepancies     & \Sexpr{papers_with_data_no_code %>% filter(Relaxed_reproducible_20 == "yes") %>% nrow()}                    & \Sexpr{papers_with_code %>% filter(Relaxed_reproducible_20 == "yes") %>% nrow()}               \\\hline
\end{tabular}
\end{table}


<<reproducibilityfig, eval=FALSE, include = FALSE, echo=FALSE, message=FALSE>>=
ifelse(!dir.exists("images"), dir.create("images"), FALSE)

palette <- c("#c9c9c9", "#f2a7b1", "#c9c9c9", 
             "#9bb6d1", "#9bb6d1", "#f2a7b1", 
             "#c9c9c9", "#c991b1", "#c991b1")

labels <- c("A", "B")

# first panel
myedges = data.frame(ID = 1:8,
                     N1 = c("Not reproduced", "Reproduced",  "Not reproduced", 
                            "Reproduced",  "No code", "Code","No data", "Data"),
                     Value = c(13,19,18,1,19,32,8,51))

myedges$N1 <- paste(myedges$N1, "\n", paste0(myedges$Value))
myedges$N2 <- c(rep("Code \n 32", 2), 
                rep("No code \n 19", 2), 
                rep("Data \n 51", 2), 
                rep(paste("All surveyed papers \n", 59), 2))


mynodes <- data.frame(ID=c(myedges$N1, paste("All surveyed papers \n", 59)), 
                      x=c(4,  4 ,  4,   4,    3,  3,  2,   2,  1), 
                      y=c(5,  1,   13,  9,   11,  3,  1,   7.5,  4))


mystyles = lapply(mynodes$y, function(n) {
  list(col = palette[n])
})

for(i in 1:length(palette)){
  mystyles[[i]]$col<-palette[i]
}

names(mystyles) <- mynodes$ID

custom.style <- riverplot::default.style()
custom.style$textcex <- 1.8

myriv <- makeRiver(nodes=mynodes, edges=myedges, node_styles=mystyles)

# second panel
myedges2 = data.frame(ID = 1:8,
                      N1 = c("Not reproduced", "Reproduced",  "Not reproduced", 
                             "Reproduced",  "No code", "Code","No data", "Data"),
                      Value = c(6,26,12,7,19,32,8,51))

myedges2$N1 <- paste(myedges2$N1, "\n", paste0(myedges2$Value))
myedges2$N2 <- c(rep("Code \n 32", 2), 
                 rep("No code \n 19", 2), 
                 rep("Data \n 51", 2), 
                 rep(paste("All surveyed papers \n", 59), 2))


mynodes2 <- data.frame(ID=c(myedges2$N1, paste("All surveyed papers \n", 59)), 
                      x=c(4,  4 ,  4,   4,    3,  3,  2,   2,  1), 
                      y=c(5,  1,   13,  9,   11,  3,  1,   7.5,  4))

mystyles2 = lapply(mynodes2$y, function(n) {
  list(col = palette[n])
})

for(i in 1:length(palette)){
  mystyles2[[i]]$col<-palette[i]
}

names(mystyles2) <- mynodes2$ID
myriv2 <- makeRiver(nodes=mynodes2, edges=myedges2, node_styles=mystyles2)


png("RiverJoint.png", units="in", res=120, height=20, width=10)
par(mfrow = c(2, 1))
plot(myriv2, default_style=custom.style, nodewidth=2, plot_area=.88) 
title(ylab="The most lenient criterion",
      cex.lab=2, line = -1)
mtext(labels[2], side = 1, cex = 2, las = 2)
plot(myriv, default_style=custom.style, nodewidth=2, plot_area=.88) 
title(ylab="The strict criterion",
      cex.lab=2, line = -1)
mtext(labels[1], side = 1, line = - 4,
      outer = TRUE, cex = 2, las = 2)
dev.off()
@

\begin{figure}[!htbp]
\centering
\includegraphics[scale=0.35,angle=-90]{images/RiverJoint.png}
\caption{Summary of the reproducibility rates according to the strict (panel A) and the most lenient (panel B) criteria of reproducibility.} \label{fig:reproducibility}
\end{figure}

<<modelingpriors, include=FALSE, echo=FALSE>>=
data <- read.csv("../data/overview_numeric_anon.csv", sep = ";", 
                 stringsAsFactors = FALSE) %>%
  rename("Data accessible" = "Data_accessible") %>%
  filter(Group == "Open data policy") %>%
  select(`Data accessible`, Preregistered, Code,
         Readme, Reproducible)

data[data=="yes"] <- 1
data[data=="no"] <- 0

data <- data %>%
    mutate_if(is.character,as.numeric)

data_filtered <- data %>%
  filter(`Data accessible` == 1)

### Bayesian model
priors <- c(prior(normal(0, 1.2), class = Intercept),          
            prior(normal(0, 1.5), class = b)) 
@

<<computepriorpred,echo=FALSE,include=FALSE,cache=TRUE>>=
set.seed(14)
model_priorpred <- brm(Reproducible ~ Readme + Preregistered + Code, 
             data = data_filtered, family="bernoulli",
             prior = priors,
             sample_prior = "only"
             )
@

<<computepriorpred2,echo=FALSE>>=
alpha_samples <- as_draws_df(model_priorpred)$b_Intercept
av_accuracy <- plogis(alpha_samples)

int_summary<-c(mean = mean(av_accuracy), quantile(av_accuracy, c(0.025, 0.975)))

beta_samples <- as_draws_df(model_priorpred)$b_Code

## the prior predicted effect of code present:
effect_code <-  plogis(alpha_samples + beta_samples) - plogis(alpha_samples)

effect_code_summary<-c(mean = mean(effect_code), quantile(effect_code,
c(0.025, 0.975)))
@

<<modeling, include=FALSE, echo=FALSE>>=
model_treatment <- brm(Reproducible ~ Readme + Preregistered + Code, 
             data = data_filtered, family="bernoulli",
             prior = priors, file = "model_inf_priors")

# computing credible interval for Code on % scale
model_treatment %>%
  spread_draws(b_Intercept, b_Code) %>%
  mutate(code=plogis(b_Intercept + b_Code)-plogis(b_Intercept)) -> x

# computing credible interval for the intercept on % scale
model_treatment %>%
  spread_draws(b_Intercept) %>%
  mutate(intercept=plogis(b_Intercept)) -> intercept
@

\subsubsection{Modeling} We investigated whether the probability of reproducing an analysis \revised{(according to both the strict and the most lenient criterion)} is affected by the availability of code, a readme file or the like, and whether the study was preregistered. This analysis was carried out via a logistic multiple regression using the \texttt{brms} package \parencite{burkner2017brms}. The absence of a particular feature was coded as 0, and presence of that feature as 1, so that the intercept of the model corresponds to the estimated probability of reproducing the analysis for which only data, but not code, readme, or preregistration are available. A $\mathcal{N}(0,1.2)$ regularizing prior \parencite{NicenboimEtAlBayes2021} was defined for the intercept; this allows the prior probability of reproducibility given no documentation, no preregistration, and no code to lie anywhere between \Sexpr{round(int_summary[2],2)*100}\% and \Sexpr{round(int_summary[3],2)*100}\% with 95\% probability (mean: \Sexpr{round(int_summary[1],2)*100}\%). The prior for the slopes was $\mathcal{N}(0, 1.5)$; this implies that the prior probability of how reproducibility could change when analysis  code is present lies between \Sexpr{round(effect_code_summary[2],2)*100}\% and \Sexpr{round(effect_code_summary[3],2)*100}\% with 95\% probability (mean: \Sexpr{round(effect_code_summary[1],2)*100}\%).

\emph{Strict criterion}. The outcomes of the analysis are shown in Table \ref{tab:logistic}: the estimated probability of reproducing analysis of a paper for which only the data set is shared (intercept) is \Sexpr{round(quantile(intercept$intercept, 0.5),2)*100}\% with a Bayesian 95\% credible interval [\Sexpr{round(quantile(intercept$intercept, 0.025),2)*100}, \Sexpr{round(quantile(intercept$intercept, 0.975),2)*100}]\%. \Copy{uncertain}{While both readme file and preregistration seem to increase the probability of reproducing an analysis, these estimates are very uncertain and also allow for 0 and negative effects, \revised{which means that the available data do not allow us to quantify whether and how these factors affect reproducibility}.} In contrast, the availability of code is associated with a big increase in probability of reproducing an analysis: the estimated increase due to the availability of code is \Sexpr{round(quantile(x$code, 0.5),2)*100}\% with 95\% CrI [\Sexpr{round(quantile(x$code, 0.025),2)*100}, \Sexpr{round(quantile(x$code, 0.975),2)*100}]\%.

<<modeling_summary, echo=FALSE,results='asis'>>=
m<-fixef(model_treatment)
m<-round(m,2)
print(xtable(m, comment=FALSE, caption = "Estimates of the log odds of reproducing an analysis according to the strict reproducibility criterion: estimating the contribution of analysis code, readme, and preregistration.", label = "tab:logistic"), math.style.negative = TRUE, type="latex", caption.placement = "top")
@


<<modelingprep2, include=FALSE, echo=FALSE>>=
data_rel <- read.csv("../data/overview_numeric_anon.csv", sep = ";", 
                 stringsAsFactors = FALSE) %>%
  rename("Data accessible" = "Data_accessible") %>%
  filter(Group == "Open data policy") %>%
  select(`Data accessible`, Preregistered, Code,
         Readme, Relaxed_reproducible_20)

data_rel[data_rel=="yes"] <- 1
data_rel[data_rel=="no"] <- 0

data_rel <- data_rel %>%
    mutate_if(is.character,as.numeric)

data_filtered_rel <- data_rel %>%
  filter(`Data accessible` == 1)

@

<<modeling2, include=FALSE, echo=FALSE>>=
model_treatment_rel <- brm(Relaxed_reproducible_20 ~ Readme + Preregistered + Code, 
                       data = data_filtered_rel, family="bernoulli",
                       prior = priors, file = "model_inf_priors_relaxed")

# computing credible interval for Code on % scale
model_treatment_rel %>%
  spread_draws(b_Intercept, b_Code) %>%
  mutate(code=plogis(b_Intercept + b_Code)-plogis(b_Intercept)) -> x_rel

# computing credible interval for the intercept on % scale
model_treatment_rel %>%
  spread_draws(b_Intercept) %>%
  mutate(intercept=plogis(b_Intercept)) -> intercept_rel
@

\revised{\emph{The most lenient criterion}. The outcomes of the analysis are shown in Table \ref{tab:logistic2}: compared to \Sexpr{round(quantile(intercept$intercept, 0.5),2)*100}\% under the strict criterion, the estimated probability of reproducing analysis for which only the data set is shared is increased to \Sexpr{round(quantile(intercept_rel$intercept, 0.5),2)*100}\% with a Bayesian 95\% credible interval [\Sexpr{round(quantile(intercept_rel$intercept, 0.025),2)*100}, \Sexpr{round(quantile(intercept_rel$intercept, 0.975),2)*100}]\%. Interestingly, the availability of code is associated with the same increase in probability of reproducing an analysis as under the strict criterion: the estimated increase is \Sexpr{round(quantile(x_rel$code, 0.5),2)*100}\% with 95\% CrI [\Sexpr{round(quantile(x_rel$code, 0.025),2)*100}, \Sexpr{round(quantile(x_rel$code, 0.975),2)*100}]\%.}


<<modeling_summary2, echo=FALSE,results='asis'>>=
m<-fixef(model_treatment_rel)
m<-round(m,2)
print(xtable(m, comment=FALSE, caption = "Estimates of the log odds of reproducing an analysis according to the most lenient reproducibility criterion: estimating the contribution of analysis code, readme, and preregistration.", label = "tab:logistic2"), math.style.negative = TRUE, type="latex", caption.placement = "top")
@


\section{Discussion}

Although the reproducibility rate \revised{according to the strict criterion (\Sexpr{100 * round(sbst %>% filter (Reproducible == "yes") %>% nrow() / sbst  %>% nrow(),2)}\%)} may seem discouraging, our estimate is not very different from those reported in other reproducibility attempts \revised{that used similarly strict reproducibility criteria} (Table~\ref{tab:reprotable}). \revised{No reproducibility attempts used exactly the same criteria, so the outcomes cannot be directly compared. Nevertheless, it is useful to see what the numbers are like across such attempts, keeping in mind that there are important differences.} \textcite{stodden2018empirical}, who analyzed the  reproducibility of papers published in \emph{Science} after the data sharing policy took effect, report an estimated reproducibility of 26\%. \textcite{hardwicke2018data}, who assessed the reproducibility of papers published in \emph{Cognition}, estimated a reproducibility rate of 31\%  without author assistance, and an additional increase of 31\% when the authors of the original manuscript helped to reproduce the outcomes. Similarly, \textcite{hardwicke2021analytic} report a reproducibility rate of 36\% without author assistance, and an increase of 24\% with author assistance for the articles published in \emph{Psychological Science} that received an `open data badge'. Similar reproducibility rates were reported for political science \parencite{eubank2016lessons, stockemer2018data} and economics \parencite{chang2015economics}.

\begin{table}[!htbp] 
\caption{Summary of reproducibility rates in published investigations.}  \label{tab:reprotable} \resizebox{\textwidth}{!}{% 
\begin{tabular}{lllll} 
\hline
Paper & Reproduced / total N & Percentage reproduced (95\% CI) \\
\hline
\textcite{stodden2018empirical} & 21 (extrapolated to 53) / 204$^{*}$ & 26 {[}20, 32{]}\% \\ 
\textcite{hardwicke2018data} & 11 / 35 & 31 {[}17, 51{]}\% \\ \textcite{hardwicke2021analytic} & 9 / 25 & 36 {[}20, 59{]}\% \\ \textcite{obels2020analysis} & 21 / 36 & 58 {[}39, 72{]}\% \\ \textcite{eubank2016lessons} & 4 / 24 & 17 {[}4, 29{]}\% \\ \textcite{stockemer2018data} & 32 / 70 & 46 {[}33, 56{]}\% \\ \textcite{chang2015economics} & 22 / 59 & 37 {[}24, 49{]}\% \\
\revised{\textcite{artner2020reproducibility}} & \revised{163 / 232 (46 articles)}  & \revised{70 {[}64 ,  75{]}\% }\\
\textcite{naudet2018data} & 14 / 17 & 82 {[}53, 94{]}\% \\\hline
\end{tabular}% 
}
\vspace{1ex}

\begin{small}
{\raggedright The asterisk (*) marks the fact that 56 articles were deemed reproducible by preliminary inspection. Of these, 22 were tested, and 21 were successfully reproduced. The reproducibility rate of 26\% is an extrapolation to the untested articles. \par}
\end{small}
\end{table}

<<bootstrapping_our_reproducibility, echo=FALSE>>=
## Computing confidence intervals using bootstrapping:

meanfun <- function(data, i){
  d <- data[i, ]
  return(mean(d))   
}

# our estimate
papers_code <- data %>%
  filter(`Data accessible` == 1 & Code == 1)

bo <- boot(papers_code[ , "Reproducible", drop = FALSE],
           statistic=meanfun, R=5000)
boot_lower <- boot.ci(bo, conf=0.95, type="bca")[4]$bca[4]
boot_upper <- boot.ci(bo, conf=0.95, type="bca")[4]$bca[5]

# Obels et al. estimate
Reproducible <- c(rep(0, 15), rep(1, 21))
obels <- data.frame(Reproducible)
bo_O <- boot(obels[ , "Reproducible", drop = FALSE],
           statistic=meanfun, R=5000)
boot_lower_O <- boot.ci(bo_O, conf=0.95, type="bca")[4]$bca[4]
boot_upper_O <- boot.ci(bo_O, conf=0.95, type="bca")[4]$bca[5]


# Artner et al. estimate (using bootstrapping):
Reproducible <- c(rep(0, 69), rep(1, 163))
artner <- data.frame(Reproducible)
bo_A <- boot(artner[ , "Reproducible", drop = FALSE],
           statistic=meanfun, R=5000)
boot_lower_A <- boot.ci(bo_A, conf=0.95, type="bca")[4]$bca[4]
boot_upper_A <- boot.ci(bo_A, conf=0.95, type="bca")[4]$bca[5]
@

Only \revised{two studies} assessing the reproducibility of publications in psychology report much higher estimates: \textcite{obels2020analysis} report 58\% and \revised{\textcite{artner2020reproducibility} -- 70\% success rate}. \Copy{comparison1}{We believe that \revised{several} factors may contribute to the difference in estimates: first, \revised{there is likely a sampling bias}. \citeauthor{obels2020analysis} assessed the reproducibility of registered reports \parencite[see][]{nosek2014registered}, and \revised{\textcite{artner2020reproducibility} -- the reproducibility of the papers whose authors shared data upon request. \textcite{wicherts2011willingness} suggest that willingness to share data could be associated with the strength of evidence and the quality of reporting of statistical results. Second, \citeauthor{artner2020reproducibility} were able to reproduce 70\% of target values only after exploring every possible analysis, including those in conflict with the reported analysis procedure, and investing 280 workdays into reproducing 232 reported values}. Finally, \citeauthor{obels2020analysis} attempted to reproduce only those papers that shared the analysis code.} \label{comparisononelabel} The authors report that 58\% (36 out of 62 papers that shared the data) shared the code, which is close to our estimate of \Sexpr{100 * round(papers_with_code %>% nrow() / papers_with_data %>% nrow(), 2)}\%. In our sample, the presence of analysis code increased  reproducibility, and the reproducibility rate for papers with code was very close to the one reported by \citeauthor{obels2020analysis}: our estimate was \Sexpr{round(mean(papers_code$Reproducible),2)*100}\% (95\% CI [\Sexpr{round(boot_lower,2)*100}, \Sexpr{round(boot_upper,2)*100}]\%), and the \citeauthor{obels2020analysis} estimate was 58\% (95\% CI [\Sexpr{round(boot_lower_O,2)*100}, \Sexpr{round(boot_upper_O,2)*100}]\%).\footnote{Because \citeauthor{obels2020analysis} did not report confidence intervals, we report bootstrapped intervals here for both our and the \citeauthor{obels2020analysis} estimates.} Clearly, reproducing only those studies that share analysis code is likely to increase the estimate of reproducibility.  

\Copy{comparison2}{\revised{Despite these limitations, we can still make a crude comparison between our reproducibility attempt and those of others in related fields. Reproducibility rates evaluated according to the strict criterion are similar in our sample and in the two evaluations of psychological papers by \citeauthor{hardwicke2018data}. Given that \citeauthor{hardwicke2018data} attempted to reproduce only one key finding in each manuscript, and we evaluated all results reported in each manuscript, our lower estimate of \Sexpr{100 * round(sbst %>% filter (Reproducible == "yes") %>% nrow() / sbst  %>% nrow(),2)}\% success rate is more conservative, and the reproducibility of the surveyed papers is at least as high as that of findings in psychology.}} \label{comparison2label}

\revised{The upper bound for reproducibility in our sample, \Sexpr{100 * round(sbst %>% filter(Relaxed_reproducible_20 == "yes") %>% nrow() / sbst  %>% nrow(),2)}\%, is calculated based on the reproducibility criterion that tolerates any number of minor -- up to 10\% -- deviations and up to 20 major discrepancies between the reported and reproduced values. To our knowledge, this is the most lenient criterion of reproducibility of all previously used. And yet, the success rate of \Sexpr{100 * round(sbst %>% filter(Relaxed_reproducible_20 == "yes") %>% nrow() / sbst  %>% nrow(),2)}\% is not strikingly high. In almost half of the surveyed papers, the barriers to reproducibility were high enough to render the reported results irreproducible even when evaluated according to such a liberal criterion.}

The important questions that arise from our analysis are: What are the major barriers to reproducibility and what can we do as a field to improve reproducibility?  
  
Regarding the first question, we see four main problems: there are issues with the publicly released data, with the code, lack of documentation, and unclear terms of use. These issues are listed below.

<<countingproblems,echo=FALSE,include=FALSE,cache=TRUE>>=
sbst <- sbst %>%
  rename("Data accessible" = "Data_accessible") %>%
  filter(Group == "Open data policy") %>%
  select(-Paper.ID, -Group) %>%
  mutate(across(everything(), as.character)) %>%
  mutate(across(everything(), ~ ifelse(.x == "yes", 1, 0)))

sbst[is.na(sbst)] <- 0
@

\begin{enumerate} \label{problems}
\item \textbf{Problems with the data}:

\begin{enumerate}
\item
Not all data that is reported in the paper is shared. \revised{For \Sexpr{sbst %>% nrow()-papers_with_data %>% nrow()} papers, data sets were inaccessible. Of \Sexpr{papers_with_data %>% nrow()} papers with accessible data, only \Sexpr{papers_with_full_data %>% nrow()} (\Sexpr{100 * round(papers_with_full_data %>% nrow() / papers_with_data %>% nrow(), 2)}\%) shared all the data necessary to reproduce the reported results.} Examples of missing data:
\begin{enumerate}
\item Only aggregated data is shared for a study that analyzed unaggregated data \revised{(N=\Sexpr{sum(sbst$Only_aggregated_data)})}.
\item Five experiments are reported in the paper, but there is data for only three experiments. Or four experiments are reported, but there is data only for three of them, etc. \revised{(N=\Sexpr{sum(sbst$Experiments_missing)})}. 
\item Some predictor variables / covariates used in the analysis are not in the data file \revised{(N=\Sexpr{sum(sbst$Predictors_missing)})}. 
\item The data file that is used in the script is simply absent in the materials \revised{(N=\Sexpr{sum(sbst$Missing_data_used_in_script)})}.
\item The data set used in the paper is an openly available corpus, but the novel annotation of the corpus done by the authors was not made available \revised{(N=1)}. Without that annotation, it is impossible to reproduce the analysis. 
\end{enumerate}
\end{enumerate}

\item 
\textbf{Problems relating to the analysis code}:
\begin{enumerate}
\item Not all the code necessary for reproducing the analysis is present \revised{(N=\Sexpr{sum(sbst$Some_code_missing)})}.
\item Technical problems with the code:
\begin{enumerate}
\item The code produces an error message that we cannot debug \revised{(N=\Sexpr{sum(sbst$Cannot_debug)})}.
\item The code is outdated and does not run any more (this is sometimes referred to as code rot). This happens because the environment in which the code was created changes; e.g., the syntax used for certain functions changes fundamentally, or some needed library is no longer publicly available, etc. \revised{(N=\Sexpr{sum(sbst$Code_rot)})}. 
\item The code is written in proprietary software and cannot be executed free of charge \revised{(N=1)}.
\item \revised{The modeling code has no documentation or readme; it is unclear what parameter values allow to reproduce the reported results \revised{(N=\Sexpr{sum(sbst$Model_not_documented)})}.} 
\end{enumerate}
\end{enumerate}

\item
\textbf{Problems with documentation}:
\begin{enumerate}
\item The shared data is difficult to interpret:
\begin{enumerate}
\item No readme file clarifies what opaque variable labels stand for. In many cases, it is possible to reconstruct which column names correspond to the variables mentioned in the manuscript. In some cases, it is very hard or impossible. \revised{For three papers, missing mapping from the reported variables to column names was the main obstacle to reproducibility.}
\item The names of the data files do not correspond to how the experiments are numbered or presented in the manuscript, and there is no readme file that establishes the correspondence \revised{(N=\Sexpr{sum(sbst$Filenames_not_experiment_names)})}.
\end{enumerate}
\item The analysis is described in insufficient detail or the description contradicts the outcome of the analysis. Examples:
\begin{enumerate}
\item The manuscript claims that something was done that is not mirrored in the outcome of the analysis (e.g., the authors claim to have used X as a predictor in the model, but the table summarizing the outcome of the model does not include X as a predictor; the authors claim to have used a particular reference level for a factor variable, but the table summarizing the outcome shows that another factor level was used as a reference, etc.).
\item The authors state that they report the outcome of a stepwise elimination procedure for model selection, but forget to do so for one of the models, and without this information, the outcome cannot be reproduced.
\item Data trimming is often described very briefly and ambiguously, so that three different trimming procedures could fit the same description. Or data trimming is not mentioned at all. Often, we  could not reproduce the results following the analysis steps described in the manuscript, presumably because the authors did not report some preliminary manipulations with the data set -- trimming, centering and scaling of variables, etc. This could be easily avoided if analysis code was present.
\end{enumerate}
 
\end{enumerate}

\item 
\textbf{Unclear terms of use}: A common problem is that often no license is specified for the data and code \revised{(N=\Sexpr{sum(sbst$`Unclear.terms.of.use`)})}. The absence of a license means that data and code could be under copyright protection and may not be usable by other researchers. It is not clear to us what the status is of data that have no license or terms of use specified for it. On the one hand, if code is released and no prohibition against further use is stipulated, the code and data should be usable by a third party. On the other hand, data and code are intellectual property, and so should be released with a license. A recent development is that funding agencies have started requiring data management plans or DMPs for experimental research (for an example, see \url{https://osf.io/7c8sz/}). DMPs usually require that one specify terms of use (e.g., a waiver) or a license. \textcite{dcclicense} is a useful guide to deciding on which license to use. 
\end{enumerate}

\revised{The surveyed papers exhibit different constellations of the problems we listed. Not all of these problems completely block reproducibility, but all of them hinder it in one way or another. Moreover, for some papers, the precise cause of irreprducibility could not be identified: sometimes the reported values could not be reproduced despite having the full data set and analysis code. In other cases, the data set was published and no trimming procedure was described in the paper, but the condition means could not be reproduced. We tentatively assume that is such cases, the wrong version of either the data set or code was made public, or data and code were not updated after the manuscript had been revised.}

We turn now to the question of what steps can be taken to improve reproducibility.

\subsection{Steps that researchers can take to improve reproducibility}

An important point to appreciate is that reproducibility is inherently very difficult to achieve. As discussed above, the reasons for this can be quite mundane. The easiest way to fail to achieve reproducibility is, of course, to not provide the data or code that led to the published claims. For this reason, just providing the code along with the data will already improve the situation considerably (according to our estimates, providing code with data increases reproducibility by \Sexpr{round(quantile(x$code, 0.5),2)*100}\% (95\% CrI [\Sexpr{round(quantile(x$code, 0.025),2)*100}, \Sexpr{round(quantile(x$code, 0.975),2)*100}]\%). So, even this one minimal change will have a big impact.


Below, we provide some suggestions for improving the reproducibility of analyses.  

\begin{enumerate}
\item \textbf{Release complete data in a usable form, with preprocessing code}: \revised{Just sharing some materials related to the project is not sufficient.} Make sure that all the data necessary to reproduce the analyses is shared. For example, if reporting that a covariate influences a particular conclusion (for example, the position of the trial in the experiment, or participants' age), this variable must be present in the data set. 

Share all the data collected: the data file need not consist of the raw output from the recording device; it can have been preprocessed. This is sometimes necessary because otherwise a very large amount of data would have to be stored online. However, all trimming and data transformations (including aggregation) should be performed in a pre-processing code script, and that script should be released with the data. In this context, the ``good enough'' computing practices advocated by \textcite{wilson2017good} are worth adopting.

One reason for releasing the raw data is that some  attributes of the data set may be valuable to other researchers. For example, the trial ID, the reading times on the last word in the sentence, or the average reading speed may be of no interest in the original paper, but could be highly relevant for researchers who study adaptation effects, sentence wrap-up effects, or average reading speed \parencite{brysbaert2019many}. Public data sets can be reused in ways that the authors haven't thought about \parencites[e.g., in meta-analyses,][]{mahowald2016meta,burki2020did}. 

\item \textbf{Use publicly accessible repositories for data sharing}: Share data on a publicly accessible server that can guarantee data being available for at least 10 years. It is generally not a good idea to store data on one's university homepage, because university homepages tend to frequently migrate and change urls. When researchers themselves move from one university to another, their old pages shut down and are no longer available. Providing data and code as part of the journal's supplementary section \revised{makes the materials effectively inaccessible to those who do not have a paid subscription. In contrast to manuscripts, which can be legally accessed as preprints on archives like arXiv or PsyArXiv, or on authors personal websites, these data sets are truly inaccessible for many people.}

Most papers that we surveyed shared their data set on Open Science Framework (\url{https://osf.io/}), Mendeley data (\url{https://data.mendeley.com/}), Zenodo (\url{https://zenodo.org/}) and some open institutional repositories. Other existing solutions include the Dataverse Project (\url{https://dataverse.org/}) and Github (\url{https://github.com}).

\item \textbf{Use non-proprietary data formats}: Sometimes data files are released as Word documents, Excel sheets, or even as pdf files. It is better to use plain text or comma-separated-values (csv) files for data release \revised{because in contrast to proprietary formats, these formats do not get outdated: it might be impossible to open a word file created some 20 years ago, but plain text files can still be read.}

Storing data in Excel sheets is also problematic because Excel makes some automatic decisions behind the scenes, for example, regarding the type of the input data. Researchers may not even be aware that the representation of their data has changed. This has already led to serious mistakes: for example, several human gene names are automatically converted to dates, which affected up to one-fifth of the surveyed genomics papers \parencite{ziemann2016gene} and ultimately led to changes in the guidelines for genome naming \parencite{wain2002guidelines}; another example involves failure to report new COVID cases in the UK because an Excel spreadsheet containing lab results reached its maximum size, and failed to update for over a week.\footnote{\url{https://www.bbc.com/news/technology-54423988}}

\revised{Using Excel for data analysis is even more dangerous: Excel formulas can be scattered through several sheets and are notoriously hard to test and debug, which leads to the proliferation of errors. An example from economics is discussed in \textcite{herndon2014does}.}

\item \textbf{Provide documentation}: Write a description of the data set and variable names (usually stored in a file called ``readme'', ``codebook'' or ``data dictionary''). \revised{Don't be afraid of redundancy:} In addition to having the codebook, give variables and their values transparent and self-explanatory names. For example, if there are two conditions like subject relatives and object relatives, label the levels ``subj-rel'' and ``obj-rel'' or the like, instead of non-obvious labels like 1 and 2. Provide meta-data if the chosen storage server allows it: meta-data makes the data set more findable, which increases the chances that it will be reused and cited.

Related to documentation, the experiment item files (including fillers) should also be provided. These are usually not needed to ensure reproducibility, but can be very valuable for carrying out replication attempts.

\item \textbf{Share analysis code together with data.} If the analysis code is not shared, it becomes very difficult and time-consuming, if not impossible, to trace back how the reported results were obtained. 

A nice illustration of the cost incurred by non-reproducible code is the case recently reported by \textcite{brewer2021discrepant}: two analyses of the same data set yielded completely different results, and ``it took the authors several days of email correspondence to determine where the differences were coming from'' (the main difference was the mean-centering of predictors in one of the analyses). Two studies of reproducibility in psychology report similar estimates: ``525 person hours'' \parencite[][, p. 12]{hardwicke2018data}, and ``between 1 and 30 (median = 7, interquartile range = 5) hours'' \parencite[][, p. 5]{hardwicke2021analytic} to reproduce one key result from a published paper. Scenarios like these can be avoided by simply providing reproducible analysis code.

A further complication is that the cases described above refer to the lucky scenario when the authors of the original manuscript are responsive and actively participate in the efforts to reproduce their analyses. Often this is not the case \parencite{wicherts2006poor, vanpaemel2015we}; in addition, author responsiveness and ability to provide information decrease with time after publication \parencite{vines2014availability, minocher2020reproducibility}, mostly due to email addresses that no longer work, and to lost or otherwise inaccessible data sets. For all of these reasons, it is highly desirable that all experimental materials, such as the data set, experimental items, and data analysis code, are shared in such a way that they can be used on a stand-alone basis, not dependent on the authors being able and willing to provide clarifications.

\item \textbf{Test the code and data release for usability}: \Copy{download}{After uploading data and code to a server, try downloading it to a different folder, preferably on a different computer, and running the analysis. A common outcome of such testing is that the uploaded analysis is not yet ready for release: some code or data files are not yet in the repository, some of the required software packages are not explicitly loaded or mentioned, etc.}\label{downloadlabel} This can happen, for example, if the researchers rely on automatically saving some of the objects they created in a hidden .Rdata file, which is not available to a third-party user. After fixing the problems that arose as a result of this test, one may ask a colleague to download one's materials and reproduce the analysis, to test whether a person who is not intimately familiar with the project can understand and run the analysis.

\item \textbf{Teach the next generation to use data management and computing skills}: A root cause for the problems that researchers face in producing reproducible research is that in psychology and psycholinguistics, historically there has been no tradition of teaching beginning researchers how to develop a research workflow that includes good quality code writing practices, data management, and documentation techniques. A good place to start would be to host a Data Carpentry (\url{https://datacarpentry.org/}) or a similar workshop on data management.

\end{enumerate}

Following these suggestions should eliminate the majority of the problems that we encountered. There are many recommendations and tutorials available for developing a better workflow. For a set of simple steps that increase reproducibility, refer to \textcite{obels2020analysis} and \textcite{wilson2017good}; for a gentle introduction to data sharing steps, such as choosing a repository, preparing data and code for sharing, structuring folders, see \textcite{klein2018practical} and \url{https://www.ukrn.org/primers/}; for the illustration of how the principles of high-reliability organizations, such as aviation and medicine, can be applied in the context of a scientific lab in psychology and related areas, see \textcite{rouder2019minimizing}; for a discussion of ``Findable, Accessible, Interoperable, and Reusable'' or FAIR  principles of data sharing and their implementation in practice, see \textcite{jacobsen2020fair}; and for a more technically involved set of recommendations for organizing a data analysis workflow, see \textcite{peikert2021reproducible, peikert2021tutorial}. 
 
Some good examples of code and data release that we found during our assessment are the following: 
\begin{enumerate*}[label=(\roman*),before=\unskip{  }, itemjoin={{, }}, itemjoin*={{, and }}]
\item \textcite{boyce2020maze}:
\url{https://github.com/vboyce/Maze}
\item \textcite{siew2021syllable}:
\url{https://osf.io/adc2p/}
\item \textcite{nooteboom2020repairing}: 
\url{https://osf.io/jahqe/}
\item \textcite{gunther2020immediate}:
\url{https://osf.io/ftxjy/}.   
\end{enumerate*}

\revised{Shared data and code, unlike the text of the manuscript, can be updated even after publication. For that reason, before adopting new data sharing practices in the ongoing or future projects, researchers might want to test their already published materials for usability, and update them accordingly. Such a self-administered reproducibility check is useful for two reasons: for the researcher, it is a very informative first step highlighting their current weak spots and areas of improvement; for the scientific community, it makes the materials truly available. After all, for a researcher accessing materials ten years from now, it would not matter whether initially, some of the materials were missing; all that matters is that the materials are available at the time of access}.

One important change that researchers might optionally consider adopting in their workflow is to use automated ways to incorporate numbers and figures in a manuscript. In our attempts to reproduce the reported results, we encountered copy-and-paste errors, such as two identical rows in a table summarizing the results of a linear model, or the loss of all minus signs in a large table. Copying and pasting numbers by hand is time-consuming and error-prone. Good automated alternatives for R include excellent packages \texttt{sjPlot} \parencite{sjPlot} and \texttt{apaTables} \parencite{stanley2018reproducible} producing publication-ready summary tables for Word and html, the \texttt{kableExtra} package producing tables for \LaTeX  \parencite{zhu2019kableextra}, and the \texttt{report} package for automatic generation of both tables and texts reporting the outcomes of statistical analyses \parencite{report}. R Markdown considerably simplifies the manuscript-preparation process; packages like \texttt{papaja} \parencite{aust2018papaja} allow the researcher to automatically format the paper in APA style, and to dynamically generate tables, figures, and numbers within the manuscript. Under this literate programming style \parencite{knuth1984literate}, the manuscript also serves as a complete documentation of the data analysis workflow. The present paper is written using a literate programming style.

\subsection{Potential concerns to developing a reproducible workflow}

Here, we address some concerns that researchers might have to developing a reproducible workflow. 

A concern \revised{shared by 22\% of psychology researchers \parencite{borghi2021data}} is that writing code and documentation in a publicly accessible way will be very time-consuming and will take time away from core scientific work. One may also lose time if one has to acquire new data management and coding skills (e.g., learning R Markdown for document preparation). 

There is no denying that preparing the data set and analysis code for archiving is indeed costly. Three considerations may help here. First, if the researcher adopts a reproducible workflow, the initial time investment is likely to be fully repaid during the active phase of working on a project. For example, the entire analysis for a slightly changed data file can be carried out by simply re-running the analysis script. If one adopts a literate programming style, all the tables, figures, and in-text numbers are generated automatically. \Copy{preparing}{Second, when preparing the data and code for public release, the researcher \revised{is likely to pay more attention to the analysis and} may catch errors that may otherwise have gone unnoticed.} \revised{Catching such errors and ensuring the reliability of published findings is core scientific work, and is worth the time cost.} Finally, if, as required by many journals, code and data are released during the review process, mistakes can be caught before the paper is published. For example, in one case \parencite{jager2020interference}, a reviewer (Brian Dillon) examined the code and found an error in the code while reviewing the paper; the error was corrected in the revision. If the authors hadn't provided the code and data, this error could have gone undetected.



Another potential concern is: will anyone ever look at the published data and code? If not, why invest time and effort into releasing these? It is indeed impossible to know whether other researchers will engage with one's data, but if the data and code are not shared, it is guaranteed that nobody will use it, because nobody can. The easier a data set is to access publicly, the higher the chance that it will be reused. In addition, as mentioned earlier, (some aspects of) the data set may turn out to be useful for answering new research questions, and for evidence synthesis through meta-analysis \parencites{VasishthetalPLoSOne2013,mahowald2016meta,JaegerEngelmannVasishth2017,NicenboimPreactivation2019,burki2020did,cox2022bayesian}. \revised{Finally, the authors themselves may wish to revisit the published data. It is therefore useful to treat the published materials as a personal back-up. From this point of view, preparing the data set and the analysis code is a courtesy to the researcher's future self who will not need to spend days looking for data and trying to understand which file contains the relevant information in ten years' time.}

Several surveys of data sharing practices show that less experienced researchers are hesitant to share data and analysis code out of fear of public shaming, loss of reputation, etc., if any errors are found \parencite{meteyard2020best, soeharjono2021reported}. It is possible that errors may be found, but this does not necessarily lead to an adverse effect on one's career \parencite{ebersole2016scientists}. While we cannot speak for all researchers, in our experience, usually those who are good at spotting mistakes are good at it precisely because they found a lot of mistakes in their own code, and therefore tend to be more understanding towards mistakes in other people's code. The reality is that everyone makes mistakes; it is not a personal failure on a researcher's part when a mistake is discovered. 

One strategy for overcoming the fear of having an error detected publicly is to ask a trusted colleague for feedback before releasing the data and code; this is no different than asking for feedback on a to-be-submitted manuscript. One can go one step further and organize a code review group within one's lab, similarly to writing groups that exist in many research groups. Finally, while mistakes in the code are traditionally associated with compromising the main theoretical claim of the work, this is by no means the most common outcome. \Copy{support}{In our attempts to reproduce the published analyses, we repeatedly found effects that supported the main theoretical claim of the manuscript but were not reported by the authors \revised{(such as a bigger or significant effect of interest in the model for which it was reported as smaller or non-significant)}}. In other words, publishing and reviewing code can strengthen the theoretical claims of the paper instead of casting doubt on them.

In sum, the practices suggested above do come with a price, but we believe that this is a price worth paying both for the benefit of individual researchers and for the global benefit of the scientific community. Reproducibility is important and worth aiming for because it increases trust in science, and enables scientists to feel more confident about what they learned from previous work. This is a very valuable outcome for everyone, and the extra effort required to approach reproducibility will help all researchers. 

\subsection{Changes that journals can make}

Journals are already actively instituting policies that are fostering positive change. Below, we list some further changes that journals can make. 

\begin{enumerate} 
\item Make it obligatory to share analysis code along with the data, unless there are compelling reasons not to do so. \revised{The option to write that ``the data will be provided on request'' should, as a rule, not be allowed, because authors generally do not release data on request.}
\item Stipulate a clear section in the paper in which the link to the materials should always appear. Currently, the links can be found all across the paper text, from the first mention of the data set in the Methods section to Acknowledgements. In several papers the official research data statement was ``Data not available / Data will be made available on request'', but the link to the data set was in fact found in some place in the paper.
\item Have an editorial assistant check the technical requirements upon submission. Specifically, the assistant should check that:
\begin{enumerate}
\item the link indeed leads to the data and code;
\item a license is specified that allows some kind of reuse.
\end{enumerate}
\item \Copy{license}{\revised{For the data sets published on the journal website along with the papers, provide an obligatory slot for licensing information.}}
\item \revised{JML's submission portal automatically renames files. Changed file names make it difficult to establish which data file corresponds to which experiment. The submission portal should never rename data and code files.}
\item \revised{Provide a data-sharing check-box at the resubmission stage: the authors should confirm that the shared materials and code are updated and match the revised manuscript.}
\item Provide in-house assessment of reproducibility \parencite[for a discussion and analysis of the efficiency of such measures, see][]{eubank2016lessons, sakaluk2014analytic}. Many journals, like \textit{Language} and \textit{Glossa Psycholinguistics}, are planning to have or already have statistical consultants on their editorial board.
\item Engage reviewers who are part of the The Peer Reviewers' Openness (PRO) Initiative \parencite{morey2016peer} and who therefore will further ensure that open science practices are implemented.
\item Carry out periodic evaluations like the present one to evaluate whether the policies are working.
\end{enumerate}

Many problems that we have listed block the very beginning of the data analysis pipeline (missing data, missing variable descriptions, unclear data selection and trimming procedures), so it is possible that our suggestions, while helping to solve these problems, would expose other problems later in the pipeline that would call for different solutions and quality control procedures. For example, we only considered the reproducibility of the results under the analysis chosen by the authors of the original manuscript. A more challenging assessment of reproducibility would be to test if the key results hold under different, equally well-motivated analyses \parencite[for an example of such assessment, see][]{silberzahn2018many, Breznau2021observing}. The results that could pass such a test, and be independently replicated, will also serve to contribute to increased reliability in science. 


\section{Conclusion}
We assessed data availability for 59 papers published in Journal of Memory and Language before and 59 papers published after a mandatory data sharing policy was adopted. \revised{The new policy has increased the rate of data sharing by more than 50\%: \revised{\Sexpr{papers_with_data_pp %>% nrow()} (\Sexpr{100 * round(papers_with_data_pp %>% nrow() / pre_policy %>% nrow(), 2)}\%) papers in the pre-policy group vs.\ \Sexpr{papers_with_data %>% nrow()} (\Sexpr{100 * round(papers_with_data %>% nrow() / sbst %>% nrow(), 2)}\%) papers in the post-policy group} had downloadable data sets. Of the 59 papers published under the data sharing policy that we investigated} we were able to fully reproduce \Sexpr{papers_with_data %>% filter (Reproducible == "yes") %>% nrow()} (\Sexpr{100 * round(papers_with_data %>% filter (Reproducible == "yes") %>% nrow() / sbst  %>% nrow(),2)}\%). \revised{Even under a very lenient reproducibility criterion that tolerates any number of small -- up to 10\% -- differences and up to 20 large-scale discrepancies between the reproduced and the reported values, the success rate was only \Sexpr{100 * round(papers_with_data %>% filter(Relaxed_reproducible_20 == "yes") %>% nrow() / sbst  %>% nrow(),2)}\%. Two simple steps can significantly increase the reproducibility of published papers: first, the authors should share the analysis code along with the data; and second, they should try to download the shared materials and attempt to reproduce their own analyses using only the shared data and code.} Such a simple self-administered reproducibility check, similar to the proofreading of a manuscript, should be a part of normal research workflow. 


\section{Supplementary materials}

The code and anonymized data for regenerating this paper are available from \url{https://osf.io/3bzu8/}. 

The detailed anonymized attempts to reproduce the 59 papers published under open data policy are available from \url{https://osf.io/3x2y6/}. The table establishing the correspondence between anonymized IDs and paper titles can be found at the root folder of the same project.

\section{Acknowledgements}

This work was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation), project number 317633480, SFB 1287. We are grateful to Dario Paape and Dorothea Pregla for comments on a draft.

\printbibliography

\end{document}

